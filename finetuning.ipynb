{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSHlAbqzDFDq",
    "tags": []
   },
   "source": [
    "# Finetuning Newton-7b instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 wandb torch==2.0.1 python-dotenv pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install -q accelerate==0.27.2 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 wandb torch==2.0.1 python-dotenv pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install -U accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.27.132 requires PyYAML<5.5,>=3.10, but you have pyyaml 6.0.1 which is incompatible.\n",
      "botocore 1.29.132 requires urllib3<1.27,>=1.25.4, but you have urllib3 2.2.1 which is incompatible.\n",
      "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.26.4 which is incompatible.\n",
      "sagemaker 2.154.0 requires attrs<23,>=20.3.0, but you have attrs 23.2.0 which is incompatible.\n",
      "sagemaker 2.154.0 requires protobuf<4.0,>=3.1, but you have protobuf 4.25.3 which is incompatible.\n",
      "sagemaker 2.154.0 requires PyYAML==5.4.1, but you have pyyaml 6.0.1 which is incompatible.\n",
      "sagemaker-training 4.5.0 requires protobuf<=3.20.3,>=3.9.2, but you have protobuf 4.25.3 which is incompatible.\n",
      "smdebug 1.0.34 requires protobuf<=3.20.3,>=3.20.0, but you have protobuf 4.25.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 wandb torch==2.0.1 python-dotenv pyyaml --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.0.1+cu117\n",
      "CUDA version: 11.7\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlpodo\u001b[0m (\u001b[33mdeepvizlab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./tmp/wandb/run-20240528_093810-sigd9lte</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/deepvizlab/finetuning/runs/sigd9lte' target=\"_blank\">finetuning-100</a></strong> to <a href='https://wandb.ai/deepvizlab/finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/deepvizlab/finetuning' target=\"_blank\">https://wandb.ai/deepvizlab/finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/deepvizlab/finetuning/runs/sigd9lte' target=\"_blank\">https://wandb.ai/deepvizlab/finetuning/runs/sigd9lte</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/deepvizlab/finetuning/runs/sigd9lte?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fc597b05d80>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    EarlyStoppingCallback, \n",
    "    IntervalStrategy\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "load_dotenv(\"config.env\")\n",
    "\n",
    "with open('config.yaml') as file:\n",
    "    config= yaml.safe_load(file)\n",
    "    #print(config['hugginfaces']['model_name'])\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "\n",
    "login(token='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')\n",
    "os.environ[\"WANDB_PROJECT\"] = \"finetuning\"\n",
    "os.environ[\"WANDB_NAME\"]= \"finetuning-100\"\n",
    "os.environ[\"WANDB_API_KEY\"] = \"e487be984a9468fa0fefabb74883fd81beed289f\"\n",
    "wandb.login()\n",
    "wandb.init(dir=config['cache_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
    "    customize this function to train the model on its own dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (`str`):\n",
    "            The name of the dataset to be loaded.\n",
    "\n",
    "    Returns:\n",
    "        dataloader (`torch.utils.data.DataLoader`):\n",
    "            The dataloader for the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def add_eos_token(sample):\n",
    "        sample['text'] = '<s>' + sample['text']+ '</s>'\n",
    "        return sample\n",
    "\n",
    "    dataset = dataset.map(add_eos_token)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUWGAP4CVRoI",
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#test = load_dataset(dataset_location, split='test[:5%]', revision=\"3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zeTrpVjFNuHR",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 546/546 [00:00<00:00, 2.58MB/s]\n",
      "Downloading data: 100%|██████████| 1.82M/1.82M [00:00<00:00, 5.04MB/s]\n",
      "Downloading data: 100%|██████████| 349k/349k [00:00<00:00, 1.07MB/s]\n",
      "Generating train split: 100%|██████████| 12570/12570 [00:00<00:00, 74883.29 examples/s]\n",
      "Generating test split: 100%|██████████| 2455/2455 [00:00<00:00, 39742.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# dataset = load_dataset(\"DeepvizLab/100-v2\", \n",
    "#                        #split=\"train\",\n",
    "#                        revision=config['hugginfaces']['dataset']['version'],\n",
    "#                        download_mode=\"force_redownload\",\n",
    "#                        cache_dir=config['cache_dir'])\n",
    "dataset = load_dataset(\"LucaPodo/newton-dataset-v1\", revision=\"0.0.2\",download_mode=\"force_redownload\", cache_dir=\"./tmp\" )\n",
    "\n",
    "#dataset = build_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 12570/12570 [00:00<00:00, 22073.59 examples/s]\n",
      "Map: 100%|██████████| 2455/2455 [00:00<00:00, 14860.00 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train = build_dataset(dataset['train'])\n",
    "test = build_dataset(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"<s>Medium @ Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n### Instruction:\\nBar chart x axis product name y axis how many product name , rank by the Y-axis in desc .\\n### Input:\\n[('product_id', 'numeric'), ('product_type_code', 'categorical'), ('product_name', 'categorical'), ('product_price', 'categorical')]\\n### Response:\\nmark bar data products encoding x product_name y aggregate count product_name transform group x sort y desc\\n</s>\"}\n"
     ]
    }
   ],
   "source": [
    "print(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 12570/12570 [00:00<00:00, 22195.60 examples/s]\n",
      "Map: 100%|██████████| 2455/2455 [00:00<00:00, 15102.07 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n### Instruction:\\nBar chart x axis product name y axis how many product name , rank by the Y-axis in desc .\\n### Input:\\n[('product_id', 'numeric'), ('product_type_code', 'categorical'), ('product_name', 'categorical'), ('product_price', 'categorical')]\\n### Response:\\nmark bar data products encoding x product_name y aggregate count product_name transform group x sort y desc\\n</s>\", 'hardness': '<s>Medium'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 12570/12570 [00:00<00:00, 153627.21 examples/s]\n",
      "Filter: 100%|██████████| 2455/2455 [00:00<00:00, 78225.19 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n### Instruction:\\nBar chart x axis product name y axis how many product name , rank by the Y-axis in desc .\\n### Input:\\n[('product_id', 'numeric'), ('product_type_code', 'categorical'), ('product_name', 'categorical'), ('product_price', 'categorical')]\\n### Response:\\nmark bar data products encoding x product_name y aggregate count product_name transform group x sort y desc\\n</s>\", 'hardness': '<s>Medium'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_hardness(sample):\n",
    "    sample['hardness'] = sample['text'].split(\"@\")[0].strip()\n",
    "    text = sample['text'].split('@')[1]\n",
    "    sample['text'] = '<s>' + text\n",
    "    return sample\n",
    "\n",
    "train = train.map(extract_hardness, batched=False)\n",
    "test = test.map(extract_hardness, batched=False)\n",
    "\n",
    "print(train[0])\n",
    "\n",
    "train = train.filter(lambda ds: ds['hardness'] == \"<s>Easy\" or ds['hardness'] == \"<s>Medium\")\n",
    "test = test.filter(lambda ds: ds['hardness'] == \"<s>Extra Hard\" or ds['hardness'] == \"<s>Extra Hard\")\n",
    "\n",
    "print(train[0])\n",
    "\n",
    "train = train.select(range(200)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n### Instruction:\\nBar chart x axis product name y axis how many product name , rank by the Y-axis in desc .\\n### Input:\\n[('product_id', 'numeric'), ('product_type_code', 'categorical'), ('product_name', 'categorical'), ('product_price', 'categorical')]\\n### Response:\\nmark bar data products encoding x product_name y aggregate count product_name transform group x sort y desc\\n</s>\", 'hardness': '<s>Medium'}\n"
     ]
    }
   ],
   "source": [
    "print(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Bar chart x axis product name y axis how many product name , rank by the Y-axis in desc .\n",
      "### Input:\n",
      "[('product_id', 'numeric'), ('product_type_code', 'categorical'), ('product_name', 'categorical'), ('product_price', 'categorical')]\n",
      "### Response:\n",
      "mark bar data products encoding x product_name y aggregate count product_name transform group x sort y desc\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "print(train[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# # Function to remove prefix\n",
    "# # def remove_prefix(text):\n",
    "# #     return re.sub(r'^<s>\\s*\\w+ @ ', '<s>', text)\n",
    "\n",
    "\n",
    "\n",
    "# # Define the function to process the dataset\n",
    "# def remove_hardness(sample):\n",
    "#     # Remove the prefix from the 'text' field\n",
    "#     sample = sample['text']\n",
    "#     return sample\n",
    "\n",
    "# # Apply the function to the dataset\n",
    "# train = train.map(remove_hardness, batched=False)\n",
    "\n",
    "# # Print the first few entries of the processed dataset\n",
    "# print(\"First few entries:\")\n",
    "# for example in train.select(range(2)):\n",
    "#     print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "e108ea2eaa74441584b7d97f8cb5b4b0",
      "c9d403a79efe440ea406df0f14b6bf3a",
      "760cd62a2f294669906b187d5e000014",
      "a68a8ab6cae14d31a45a3fcb0dfa360a",
      "1bfd12f68ae743619975e9ceb5555731",
      "99e9d9e65254495781e539c609b72d9a",
      "36e6cc28aad547bd946aeb8d39b79337",
      "e1a6d4eda937414fb164c8d423042dd6",
      "7c75d273e20746e18300976960f4edd4",
      "c757bae880ea46e3a38466f91e7ad978",
      "23169670cc0444b8a12d525fef80f445",
      "85c4fbc1751d4f35b6d81ea5278755cb",
      "58ad610d75fe497ba65de6f683bd0e8a",
      "294742b46534486a9ca3639c0c82bf5a",
      "0445a34071a94fd79e9b3a275aeb52a8",
      "bbec3814deb64a20ba516a4ce7e1af74",
      "cc6d3ba0ab094b73bd3434370f090541",
      "d22ac6cfb47f461d8998fde86bd74e03",
      "3e5714f3beba4983b3b9b94f756bf2b0",
      "4d5cc654034744fd95a4172f26e395ec",
      "faf8cb772bfd450ea58759c01f7639c9",
      "2505f5dfe8f745f9a13e34c64779431c",
      "a773d3278cb541649809e6f7649a4c1c",
      "a0c251b4d419418780232d9bf4f49d02",
      "1888fb4d5f5344428e14a917e57cdd26",
      "cdabdc345d9b492db6cbce0352de53ea",
      "b18fafbe62414ec691418946c27b448b",
      "ce57580b70ea442bada9525e80d84fee",
      "108bfc13e5084ca9ad7cafdff20b6d22",
      "a1555c2e942b43cc9c4941b06efb76f4",
      "f853a83267f94aa18765390a0ecb80f3",
      "f917768aaaf54d76b33aa96b9997d563",
      "07023caa7e494a13b8ea3853141b8089",
      "7b51d25886d740b2a8d259e091cffc51",
      "c2d69dbc2d2747c6bfae824ea80a6877",
      "befce5edb9ba418db491843ac5b921f4",
      "3bff36ea638c497b8badaf288a433186",
      "e8718f3d22f345bbbdc7198c208b9434",
      "e763ee0f53034737ab315124de0198ed",
      "6aad5767036d47928641666fc07b0f92",
      "d61e5317d07146be8b510da6546737e3",
      "9f2e0b28cc924db4b9c5ed7467c1e93a",
      "cf783c0a1a584212a0238de5d23a8b33",
      "fc557c3c7c1044fc9add53af9a54ab92",
      "101c4337217c42f7a8a00115e4b6076f",
      "d81a089b6a5a4baca816e85e1241b28e",
      "e68c84509fae4c5099d868ea9421c32a",
      "535405eff48b44b082b72267f0c3c03d",
      "a10ba1a1d89e4e53ad40e3d5ef763389",
      "1ba925fbf1344a42b04151a5f4ffc6b9",
      "f9f4d47e990f434aaa1d9e97665dcdcf",
      "3832410f12b945718d5c4b8125bc38ce",
      "46f3de843e0b4521b71907c7334d4d66",
      "e167447c37134eeea58cadedab3a35a4",
      "3ba8ae2512574c36b5b2e40da4259935",
      "5f16c6c4bf9a420c841afa5e73796750",
      "fb813be5851040a6a6128e5518708c69",
      "294d1b701cc84c0bac60cfdd4f689bd1",
      "bbd7f8bb1d7d4900a30c5012a719f7b3",
      "d18ee99d144c4422b90ca29b58e5ffd4",
      "a24d2a5732ea47178c551efd6e60699f",
      "4a458335c4854c049375ab5b0ef01e6e",
      "c6840e0cbd2d406d9a951a86a1ee1566",
      "fa6baf7521c94b37972ff1bac316eea0",
      "35a91bf3f3f744aca835c793000a62ce",
      "992d929340764f99ad129bb514742c9a",
      "42ca301f5a344713b7d487aa08100ed0",
      "29013e03099941c29edff7078c068755",
      "98b94c534ec844d1996fcb9df8d2b52f",
      "a67ba78885c8440cba2311363aaf36ae",
      "738e9b88f0654309b6c8f2b88a4cca64",
      "8de2c15200374452a8af14524d5d7441",
      "2bf0c972356349169a9add8e60b5203f",
      "91623e83138b4f369154fc70b403798e",
      "89b99b3d48634e0e8c3528f358ffb5a7",
      "add020a8e0724b64a096527d921204fd",
      "80e216e451894eb0bc846f16c2ed0514"
     ]
    },
    "id": "OJXpOgBFuSrc",
    "outputId": "591df792-8985-4786-d856-a1b37201f083",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:53<00:00, 26.63s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, config['bitsandbytes']['bnb_4bit_compute_dtype'])\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=config['bitsandbytes']['use_4bit'],\n",
    "    bnb_4bit_quant_type=config['bitsandbytes']['bnb_4bit_quant_type'],\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=config['bitsandbytes']['use_nested_quant'],\n",
    ")\n",
    "\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and config['bitsandbytes']['use_4bit']:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config['fine_tuning']['model']['ref_model_name'],\n",
    "    quantization_config=bnb_config,\n",
    "    device_map = {\"\": 0}\n",
    ")\n",
    "model.config.use_cache = config['fine_tuning']['model']['use_cache']\n",
    "model.config.pretraining_tp = config['fine_tuning']['model']['pretraining_tp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS Token: 1\n",
      "EOS Token: 2\n",
      "PAD Token: 32000\n",
      "['<s>', '</s>', '<unk>', '[PAD]']\n",
      "Without special tokens:\n",
      "Tokenized Text: ['<s>', '', 'Below', 'is', 'an', 'instruction', 'that', 'describes', 'a', 'task', ',', 'pa', 'ired', 'with', 'an', 'input', 'that', 'provides', 'further', 'context', '.', 'Write', 'a', 'response', 'that', 'appropri', 'ately', 'comple', 'tes', 'the', 'request', '.', '\\n', '##', '#', 'Inst', 'ruction', ':', '\\n', 'Bar', 'chart', 'x', 'axis', 'product', 'name', 'y', 'axis', 'how', 'many', 'product', 'name', ',', 'rank', 'by', 'the', 'Y', '-', 'axis', 'in', 'desc', '.', '\\n', '##', '#', 'Input', ':', '\\n', '[', \"('\", 'product', '_', 'id', \"',\", \"'\", 'numeric', \"'),\", \"('\", 'product', '_', 'type', '_', 'code', \"',\", \"'\", 'c', 'ategor', 'ical', \"'),\", \"('\", 'product', '_', 'name', \"',\", \"'\", 'c', 'ategor', 'ical', \"'),\", \"('\", 'product', '_', 'price', \"',\", \"'\", 'c', 'ategor', 'ical', \"')\", ']', '\\n', '##', '#', 'Response', ':', '\\n', 'mark', 'bar', 'data', 'products', 'encoding', 'x', 'product', '_', 'name', 'y', 'aggregate', 'count', 'product', '_', 'name', 'transform', 'group', 'x', 'sort', 'y', 'desc', '\\n', '</s>']\n",
      "Token IDs: [1, 29871, 13866, 338, 385, 15278, 393, 16612, 263, 3414, 29892, 3300, 2859, 411, 385, 1881, 393, 8128, 4340, 3030, 29889, 14350, 263, 2933, 393, 7128, 2486, 1614, 2167, 278, 2009, 29889, 13, 2277, 29937, 2799, 4080, 29901, 13, 4297, 8727, 921, 9685, 3234, 1024, 343, 9685, 920, 1784, 3234, 1024, 1919, 7115, 491, 278, 612, 29899, 8990, 297, 5153, 869, 13, 2277, 29937, 10567, 29901, 13, 29961, 877, 4704, 29918, 333, 742, 525, 21574, 5477, 6702, 4704, 29918, 1853, 29918, 401, 742, 525, 29883, 20440, 936, 5477, 6702, 4704, 29918, 978, 742, 525, 29883, 20440, 936, 5477, 6702, 4704, 29918, 9175, 742, 525, 29883, 20440, 936, 1495, 29962, 13, 2277, 29937, 13291, 29901, 13, 3502, 2594, 848, 9316, 8025, 921, 3234, 29918, 978, 343, 20431, 2302, 3234, 29918, 978, 4327, 2318, 921, 2656, 343, 5153, 13, 2]\n",
      "Token IDs: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config['fine_tuning']['model']['ref_model_name'],\n",
    "                                          trust_remote_code=True,\n",
    "                                          add_bos_token=False)#, add_eos_token=True, use_fast=True)\n",
    "tokenizer.add_special_tokens(config['fine_tuning']['tokenizer']['padding_token'])\n",
    "tokenizer.padding_side = config['fine_tuning']['tokenizer']['padding_side']\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "assert model.config.pad_token_id == tokenizer.pad_token_id, \"The model's pad token ID does not match the tokenizer's pad token ID!\"\n",
    "\n",
    "print(\"BOS Token:\", tokenizer.bos_token_id)\n",
    "print(\"EOS Token:\", tokenizer.eos_token_id)\n",
    "print(\"PAD Token:\", tokenizer.pad_token_id)\n",
    "print(tokenizer.all_special_tokens)\n",
    "sample_sentence = train['text'][0]\n",
    "\n",
    "# Tokenize without special tokens\n",
    "tokenized_output_no_special = tokenizer(sample_sentence, add_special_tokens=True)\n",
    "print(\"Without special tokens:\")\n",
    "print(\"Tokenized Text:\", [tokenizer.decode([x]) for x in tokenized_output_no_special[\"input_ids\"]])\n",
    "print(\"Token IDs:\", tokenized_output_no_special[\"input_ids\"])\n",
    "print(\"Token IDs:\", tokenized_output_no_special[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install accelerate==0.27.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "FkkT0skezLoG",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 2726.16 examples/s]\n",
      "Map: 100%|██████████| 221/221 [00:00<00:00, 2261.27 examples/s]\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 01:40, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.007000</td>\n",
       "      <td>1.167364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "early stopping required metric_for_best_model, but did not find eval_f1 so early stopping is disabled\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./models/200-easy-tokenizer/tokenizer_config.json',\n",
       " './models/200-easy-tokenizer/special_tokens_map.json',\n",
       " './models/200-easy-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=config['lora']['lora_alpha'],\n",
    "    lora_dropout=config['lora']['lora_dropout'],\n",
    "    r=config['lora']['lora_r'],\n",
    "    bias=config['lora']['bias'],\n",
    "    task_type=config['lora']['task_type'],\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=config['fine_tuning']['model']['output_dir'],\n",
    "    num_train_epochs= config['fine_tuning']['hyperparameters']['num_train_epochs'],\n",
    "    per_device_train_batch_size= config['fine_tuning']['hyperparameters']['per_device_train_batch_size'] ,\n",
    "    gradient_accumulation_steps= config['fine_tuning']['hyperparameters']['gradient_accumulation_steps'] ,\n",
    "    optim=config['fine_tuning']['hyperparameters']['optim'],\n",
    "    save_steps=config['fine_tuning']['hyperparameters']['save_steps'],\n",
    "    logging_steps=config['fine_tuning']['hyperparameters']['logging_steps'],\n",
    "    learning_rate=config['fine_tuning']['hyperparameters']['learning_rate'],\n",
    "    weight_decay=config['fine_tuning']['hyperparameters']['weight_decay'],\n",
    "    fp16=config['fine_tuning']['hyperparameters']['fp16'],\n",
    "    bf16=config['fine_tuning']['hyperparameters']['bf16'],\n",
    "    max_grad_norm=config['fine_tuning']['hyperparameters']['max_grad_norm'],\n",
    "    max_steps=config['fine_tuning']['hyperparameters']['max_steps'],\n",
    "    warmup_ratio=config['fine_tuning']['hyperparameters']['warmup_ratio'],\n",
    "    group_by_length=config['fine_tuning']['hyperparameters']['group_by_length'],\n",
    "    lr_scheduler_type=config['fine_tuning']['hyperparameters']['lr_scheduler_type'],\n",
    "    report_to=config['fine_tuning']['hyperparameters']['report_to'],\n",
    "    load_best_model_at_end = True,\n",
    "    evaluation_strategy = IntervalStrategy.STEPS,\n",
    "    eval_steps = 50,\n",
    "    metric_for_best_model='eval_f1'\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train,\n",
    "    eval_dataset = test,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=None,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=config['fine_tuning']['sft']['packing'],\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "trainer.model.save_pretrained(config['fine_tuning']['model']['output_dir'])\n",
    "trainer.tokenizer.save_pretrained(config['fine_tuning']['tokenizer']['output_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "#dataset_location= config['hugginfaces']['dataset']['name']\n",
    "#test = load_dataset(config['hugginfaces']['dataset']['name'],\n",
    "                    #split=\"test\", revision=\"4.0.4\", cache_dir = \"./tmp\")\n",
    "pipe = pipeline(task=\"text-generation\", model=trainer.model, tokenizer=trainer.tokenizer, max_length=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "For all employees who have the letters D or S in their first name , give me the comparison about the amount of job_id over the job_id , and group by attribute job_id by a bar chart , and could you show names in descending order ?\n",
      "### Input:\n",
      "[('employee_id', 'numeric'), ('first_name', 'categorical'), ('last_name', 'categorical'), ('email', 'categorical'), ('phone_number', 'categorical'), ('hire_date', 'temporal'), ('job_id', 'categorical'), ('salary', 'numeric'), ('commission_pct', 'numeric'), ('manager_id', 'numeric'), ('department_id', 'numeric')]\n",
      "### Response:\n",
      "mark\n",
      "\n",
      "mark bar data employees encoding x job_id y aggregate count job_id transform filter first_name like '%d%' or first_name like '%s%' group x sort x desc\n",
      "</s>\n",
      "\n",
      "<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "For all employees who have the letters D or S in their first name , give me the comparison about the amount of job_id over the job_id , and group by attribute job_id by a bar chart , and could you show names in descending order ?\n",
      "### Input:\n",
      "[('employee_id', 'numeric'), ('first_name', 'categorical'), ('last_name', 'categorical'), ('email', 'categorical'), ('phone_number', 'categorical'), ('hire_date', 'temporal'), ('job_id', 'categorical'), ('salary', 'numeric'), ('commission_pct', 'numeric'), ('manager_id', 'numeric'), ('department_id', 'numeric')]\n",
      "### Response:\n",
      "mark bar data employee encoding x job_id y aggregate count job_id transform group x sort y desc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 12\n",
    "prompt = test[i]['text'].split(\"### Response:\")[0] + \"### Response:\\nmark\"\n",
    "print(prompt + '\\n')\n",
    "groundtruth = test[i]['text'].split(\"### Response:\")[1].strip()\n",
    "print(groundtruth + '\\n')\n",
    "result = pipe(prompt)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n### Instruction:\\nFor all employees who have the letters D or S in their first name , give me the comparison about the amount of job_id over the job_id , and group by attribute job_id by a bar chart , and could you show names in descending order ?\\n### Input:\\n[('employee_id', 'numeric'), ('first_name', 'categorical'), ('last_name', 'categorical'), ('email', 'categorical'), ('phone_number', 'categorical'), ('hire_date', 'temporal'), ('job_id', 'categorical'), ('salary', 'numeric'), ('commission_pct', 'numeric'), ('manager_id', 'numeric'), ('department_id', 'numeric')]\\n### Response:\\nmark bar data employee encoding x job_id y aggregate count job_id transform group x sort y desc\\n\"}]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "frlSLPin4IJ4",
    "outputId": "793bca57-ce30-4dd1-eb75-f964dbe09a99",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "            ### Instruction:\n",
      "            How many enrolment students in each month ? Return a bar chart binning date of enrolment by month interval .\n",
      "            ### Input:\n",
      "            [('registration_id', 'numeric'), ('student_id', 'numeric'), ('course_id', 'numeric'), ('date_of_enrolment', 'temporal'), ('date_of_completion', 'temporal')]\n",
      "            ### Response:\n",
      "            mark bar data enrolment encoding x date_of_enrolment y aggregate count date_of_enrolment transform group x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "#- mark bar data * encoding x date_of_enrolment y aggregate count date_of_enrolment transform bin x by month\n",
    "#- mark bar data * encoding x date_of_enrolment y aggregate count date_of_enrolment transform bin x by month\n",
    "instruction = \"How many enrolment students in each month ? Return a bar chart binning date of enrolment by month interval .\"\n",
    "input_ = \"[('registration_id', 'numeric'), ('student_id', 'numeric'), ('course_id', 'numeric'), ('date_of_enrolment', 'temporal'), ('date_of_completion', 'temporal')]\"\n",
    "prompt = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "            ### Instruction:\n",
    "            {instruction}\n",
    "            ### Input:\n",
    "            {input_}\n",
    "            ### Response:\n",
    "            mark\"\"\"\n",
    "\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=400, eos_token_id=tokenizer.eos_token_id)\n",
    "result = pipe(prompt)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Empty VRAM\n",
    "#del model\n",
    "#del pipe\n",
    "import gc\n",
    "#del tokenizer\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Push to the hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:51<00:00, 25.63s/it]\n"
     ]
    }
   ],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config['fine_tuning']['model']['ref_model_name'],\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, config['fine_tuning']['model']['output_dir'])\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['fine_tuning']['tokenizer']['output_dir'], trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS Token: 1\n",
      "EOS Token: 2\n",
      "PAD Token: 32000\n",
      "['<s>', '</s>', '<unk>', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "# quando si modifica il modello si deve successivamente rifare il resize\n",
    "tokenizer.padding_side = config['fine_tuning']['tokenizer']['padding_side']\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(\"BOS Token:\", tokenizer.bos_token_id)\n",
    "print(\"EOS Token:\", tokenizer.eos_token_id)\n",
    "print(\"PAD Token:\", tokenizer.pad_token_id)\n",
    "print(tokenizer.all_special_tokens)\n",
    "assert model.config.pad_token_id == tokenizer.pad_token_id, \"The model's pad token ID does not match the tokenizer's pad token ID!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/200-easy-tokenizer/tokenizer_config.json',\n",
       " './models/200-easy-tokenizer/special_tokens_map.json',\n",
       " './models/200-easy-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n",
      "<IPython.core.display.HTML object>\n",
      "<IPython.core.display.HTML object>\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(config['fine_tuning']['model']['output_dir'])\n",
    "tokenizer.save_pretrained(config['fine_tuning']['tokenizer']['output_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# login(token=os.getenv('hf_write_token'))\n",
    "# model.push_to_hub(\"DeepvizLab/newton-7b-full\")\n",
    "# tokenizer.push_to_hub(\"DeepvizLab/newton-7b-full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Find the number of trains starting from each origin Plot them as bar chart , and order in asc by the y-axis .\n",
      "### Input:\n",
      "[('id', 'numeric'), ('train_number', 'numeric'), ('name', 'categorical'), ('origin', 'categorical'), ('destination', 'categorical'), ('time', 'categorical'), ('interval', 'categorical')]\n",
      "### Response:\n",
      "mark\n",
      "\n",
      "mark bar data train encoding x origin y aggregate count origin transform group x sort y asc\n",
      "\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Find the number of trains starting from each origin Plot them as bar chart , and order in asc by the y-axis .\n",
      "### Input:\n",
      "[('id', 'numeric'), ('train_number', 'numeric'), ('name', 'categorical'), ('origin', 'categorical'), ('destination', 'categorical'), ('time', 'categorical'), ('interval', 'categorical')]\n",
      "### Response:\n",
      "mark bar data train encoding x origin y aggregate count transform sort y asc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_location= \"LucaPodo/newton-dataset-v1\"\n",
    "test = load_dataset(dataset_location, split=\"test\", revision=\"4.0.4\", cache_dir = \"./tmp\")\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=400, eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "i = 100\n",
    "prompt = test[i]['text'].split(\"### Response:\")[0] + \"### Response:\\nmark\"\n",
    "print(prompt + '\\n')\n",
    "groundtruth = test[i]['text'].split(\"### Response:\")[1].strip()\n",
    "print(groundtruth + '\\n')\n",
    "result = pipe(prompt)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-north-1:243637512696:image/pytorch-2.0.0-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "lcc_arn": "arn:aws:sagemaker:eu-north-1:490853539869:studio-lifecycle-config/my-studio-lcc"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
