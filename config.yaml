hugginfaces:
    model_name: "DeepvizLab/100-v2"
    dataset:
        name: "LucaPodo/newton-dataset-v1"
        version: "0.0.2"

wandb:
    name: "fine-tuning"
    
cache_dir: "./tmp"

lora:
    lora_r: 64
    lora_alpha: 16
    lora_dropout: 0.1
    bias: "none"
    task_type: "CAUSAL_LM"

bitsandbytes:
    use_4bit: True
    bnb_4bit_compute_dtype: "bfloat16"
    bnb_4bit_quant_type: "nf4"
    use_nested_quant: False

fine_tuning:
    model:
        output_dir: "./models/200-easy"
        ref_model_name: "meta-llama/Llama-2-7b-hf"
        use_cache: False
        pretraining_tp: 1
    
    tokenizer:
        output_dir: "./models/200-easy-tokenizer"
        padding_side: "left"
        padding_token: {'pad_token': '[PAD]'}
        new_tokenizer_name: "200-easy-tokenizer"
        
    hyperparameters:
        num_train_epochs: 1
        fp16: False
        bf16: True
        per_device_train_batch_size: 4
        per_device_eval_batch_size: 4
        gradient_accumulation_steps: 1
        gradient_checkpointing: True
        max_grad_norm: 0.3
        learning_rate: 0.0002
        weight_decay: 0.001
        optim: "paged_adamw_32bit"
        lr_scheduler_type: "cosine"
        max_steps: -1
        warmup_ratio: 0.03
        group_by_length: True
        save_steps: 0
        report_to: "wandb"
        logging_steps: 25
        
    sft:
        max_seq_length: None
        packing: False
        device_map: {"": 0}
        dataset_text_field: "text"
        
    
    
    