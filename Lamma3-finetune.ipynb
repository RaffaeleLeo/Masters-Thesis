{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSHlAbqzDFDq",
    "tags": []
   },
   "source": [
    "# Finetuning Lamma 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.11/site-packages (0.27.2)\n",
      "Requirement already satisfied: peft in /opt/conda/lib/python3.11/site-packages (0.14.0)\n",
      "Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.11/site-packages (0.45.3)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.45.2)\n",
      "Requirement already satisfied: trl==0.11 in /opt/conda/lib/python3.11/site-packages (0.11.0)\n",
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.11/site-packages (0.19.7)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.3.1.post300)\n",
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (6.0.2)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.11/site-packages (from trl==0.11) (3.3.2)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.11/site-packages (from trl==0.11) (0.9.16)\n",
      "Requirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.11/site-packages (from trl==0.11) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.11/site-packages (from accelerate) (0.26.5)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.11/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.11/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.11/site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/conda/lib/python3.11/site-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /opt/conda/lib/python3.11/site-packages (from wandb) (2.10.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from wandb) (2.22.0)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.11/site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from wandb) (75.6.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /opt/conda/lib/python3.11/site-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2023.6.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=2.6->wandb) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /opt/conda/lib/python3.11/site-packages (from tyro>=0.5.11->trl==0.11) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.11/site-packages (from tyro>=0.5.11->trl==0.11) (13.9.4)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.11/site-packages (from tyro>=0.5.11->trl==0.11) (1.7.1)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /opt/conda/lib/python3.11/site-packages (from tyro>=0.5.11->trl==0.11) (4.4.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets->trl==0.11) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets->trl==0.11) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets->trl==0.11) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets->trl==0.11) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.11/site-packages (from datasets->trl==0.11) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets->trl==0.11) (3.9.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->trl==0.11) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->trl==0.11) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->trl==0.11) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->trl==0.11) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->trl==0.11) (1.18.3)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11) (2.18.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets->trl==0.11) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets->trl==0.11) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets->trl==0.11) (2024.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.11) (0.1.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp->datasets->trl==0.11) (0.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate peft bitsandbytes transformers trl==0.11 wandb torch python-dotenv pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.11/site-packages (3.3.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.11/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.26.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp->datasets) (0.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate==0.27.2 in /opt/conda/lib/python3.11/site-packages (0.27.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from accelerate==0.27.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from accelerate==0.27.2) (24.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate==0.27.2) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from accelerate==0.27.2) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.11/site-packages (from accelerate==0.27.2) (2.3.1.post300)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.11/site-packages (from accelerate==0.27.2) (0.26.5)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.11/site-packages (from accelerate==0.27.2) (0.4.5)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.27.2) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.27.2) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.27.2) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.27.2) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.27.2) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate==0.27.2) (2023.6.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface-hub->accelerate==0.27.2) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub->accelerate==0.27.2) (4.67.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate==0.27.2) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate==0.27.2) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate==0.27.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate==0.27.2) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate==0.27.2) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate==0.27.2) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate==0.27.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 23:36:53.557295: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-03 23:36:53.572905: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-03 23:36:53.577839: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-03 23:36:53.588908: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.1.post300\n",
      "CUDA version: 12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlpodo\u001b[0m (\u001b[33mdeepvizlab\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./tmp/wandb/run-20250303_233657-oibzubqc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/deepvizlab/finetuning/runs/oibzubqc' target=\"_blank\">finetuning-100</a></strong> to <a href='https://wandb.ai/deepvizlab/finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/deepvizlab/finetuning' target=\"_blank\">https://wandb.ai/deepvizlab/finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/deepvizlab/finetuning/runs/oibzubqc' target=\"_blank\">https://wandb.ai/deepvizlab/finetuning/runs/oibzubqc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/deepvizlab/finetuning/runs/oibzubqc?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f96a920bd50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    EarlyStoppingCallback, \n",
    "    IntervalStrategy\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "load_dotenv(\"config.env\")\n",
    "\n",
    "with open('config.yaml') as file:\n",
    "    config= yaml.safe_load(file)\n",
    "    #print(config['hugginfaces']['model_name'])\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "\n",
    "login(token='HUGGINGFACE_TOKEN')\n",
    "os.environ[\"WANDB_PROJECT\"] = \"finetuning\"\n",
    "os.environ[\"WANDB_NAME\"]= \"finetuning-100\"\n",
    "os.environ[\"WANDB_API_KEY\"] = \"WANDB_TOKEN\"\n",
    "wandb.login()\n",
    "wandb.init(dir=config['cache_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
    "    customize this function to train the model on its own dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (`str`):\n",
    "            The name of the dataset to be loaded.\n",
    "\n",
    "    Returns:\n",
    "        dataloader (`torch.utils.data.DataLoader`):\n",
    "            The dataloader for the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def add_eos_token(sample):\n",
    "        sample['text'] = '<s>' + sample['text']+ '</s>'\n",
    "        return sample\n",
    "\n",
    "    dataset = dataset.map(add_eos_token)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUWGAP4CVRoI",
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zeTrpVjFNuHR",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cefdaa7b6774998a4c6bed0b8e47f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(‚Ä¶)-00000-of-00001-653f5eb8e3f7e385.parquet:   0%|          | 0.00/1.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b991aff6bd43ee82ec3ecf901e12a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(‚Ä¶)-00000-of-00001-51e5da44a405016d.parquet:   0%|          | 0.00/349k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a117747bbfe84eceb1911d8b9f0f1806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/12570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f96ac834dfcc47889444c214b2ca4abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2455 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load the dataset from hugging face\n",
    "dataset = load_dataset(\"LucaPodo/newton-dataset-v1\", revision=\"0.0.2\",download_mode=\"force_redownload\", cache_dir=\"./tmp\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc241abae8e94350a853c501449b9553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d505ad54bc49799de8d3662e2c9fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2455 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = build_dataset(dataset['train'])\n",
    "test = build_dataset(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"<s>Medium @ Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n### Instruction:\\nBar chart x axis product name y axis how many product name , rank by the Y-axis in desc .\\n### Input:\\n[('product_id', 'numeric'), ('product_type_code', 'categorical'), ('product_name', 'categorical'), ('product_price', 'categorical')]\\n### Response:\\nmark bar data products encoding x product_name y aggregate count product_name transform group x sort y desc\\n</s>\"}\n"
     ]
    }
   ],
   "source": [
    "print(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7ac41472364388a316e9009decf8ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bae84101427443fba592375a34b9fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2455 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"<s>### Instruction:\\nBar chart x axis product name y axis how many product name , rank by the Y-axis in desc .\\n### Input:\\n[('product_id', 'numeric'), ('product_type_code', 'categorical'), ('product_name', 'categorical'), ('product_price', 'categorical')]\\n###Thought Process:\\n1. **Understand the data**: Identify numerical and categorical columns from the input.\\n 2. **Determine the chart type**: Choose an appropriate mark (`bar`, `line`, `point`, `arc`) based on the instruction and data type.\\n 3. **Define encoding**: Assign x-axis, y-axis, aggregation function (if applicable), and any color mapping.\\n 4. **Identify transformations**: Determine whether filtering, binning, grouping, sorting, or top-k selection is required.\\n 5. **Flatten into a Vega-Zero specification**: Convert the reasoning into the keyword-based format required by Vega-Zero.\\n\\n### Response:\\nmark bar data products encoding x product_name y aggregate count product_name transform group x sort y desc\\n</s></s>\", 'hardness': '<s>Medium'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b5c85ba28247429f29fc6f86896910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/12570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb33e00ce52445afaf41062a86e3403e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2455 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"<s>### Instruction:\\nBar chart x axis product name y axis how many product name , rank by the Y-axis in desc .\\n### Input:\\n[('product_id', 'numeric'), ('product_type_code', 'categorical'), ('product_name', 'categorical'), ('product_price', 'categorical')]\\n###Thought Process:\\n1. **Understand the data**: Identify numerical and categorical columns from the input.\\n 2. **Determine the chart type**: Choose an appropriate mark (`bar`, `line`, `point`, `arc`) based on the instruction and data type.\\n 3. **Define encoding**: Assign x-axis, y-axis, aggregation function (if applicable), and any color mapping.\\n 4. **Identify transformations**: Determine whether filtering, binning, grouping, sorting, or top-k selection is required.\\n 5. **Flatten into a Vega-Zero specification**: Convert the reasoning into the keyword-based format required by Vega-Zero.\\n\\n### Response:\\nmark bar data products encoding x product_name y aggregate count product_name transform group x sort y desc\\n</s></s>\", 'hardness': '<s>Medium'}\n"
     ]
    }
   ],
   "source": [
    "def extract_hardness(sample):\n",
    "    sample['hardness'] = sample['text'].split(\"@\")[0].strip()\n",
    "    text = sample['text'].split('@')[1]\n",
    "    sample['text'] = '<s>' + text\n",
    "    return sample\n",
    "\n",
    "def clean_training_sample(sample):\n",
    "    # Extract the hardness level\n",
    "    sample['hardness'] = sample['text'].split(\"@\")[0].strip()\n",
    "    # Extract the main text after '@'\n",
    "    text = sample['text'].split('@')[1]\n",
    "    # Keep the instruction and response parts\n",
    "    instruction_part = text.split(\"### Instruction:\")[1].split(\"### Input:\")[0].strip()\n",
    "    input_part = text.split(\"### Input:\")[1].split(\"### Response:\")[0].strip()\n",
    "    thought_process = \"1. **Understand the data**: Identify numerical and categorical columns from the input.\\n 2. **Determine the chart type**: Choose an appropriate mark (`bar`, `line`, `point`, `arc`) based on the instruction and data type.\\n 3. **Define encoding**: Assign x-axis, y-axis, aggregation function (if applicable), and any color mapping.\\n 4. **Identify transformations**: Determine whether filtering, binning, grouping, sorting, or top-k selection is required.\\n 5. **Flatten into a Vega-Zero specification**: Convert the reasoning into the keyword-based format required by Vega-Zero.\\n\"\n",
    "    response_part = text.split(\"### Response:\")[1].strip()\n",
    "    # Rebuild the cleaned text\n",
    "    intro = \"Generate a Vega-Zero unit specification based on the given dataset and visualization requirements. Identify the appropriate chart type, relevant data columns, encoding choices, and necessary transformations. Vega-Zero keeps most of the keywords of the Vega-Lite about the mapping between visual encoding channels and (transformed) data variables. It Ô¨Çattens a JSON object into a sequence of keywords by removing structure-aware symbols such as brackets, colons, and quotation marks. Formally, a unit speciÔ¨Åcation in Vega-Zero is a four tuple (similar to Vega-Lite but with each tuple being a sequence) as: unit¬†= (mark,¬†data,¬†encoding,¬†transform) Naturally, as a simpliÔ¨Åcation of Vega-Lite: mark¬†denotes the chart type, including¬†bar,¬†line,¬†point¬†(for scatter chart),¬†arc¬†(for pie chart); data¬†speciÔ¨Åes the source data; encoding¬†contains¬†x/y-axis,¬†aggregate¬†function, and¬†color¬†based on which column; transform¬†deÔ¨Ånes some data transformation functions:¬†Ô¨Ålter,¬†bin,¬†group,¬†sort, and¬†top-k.\"\n",
    "    sample['text'] = f\"<s>### Instruction:\\n{instruction_part}\\n### Input:\\n{input_part}\\n###Thought Process:\\n{thought_process}\\n### Response:\\n{response_part}</s>\"\n",
    "    return sample\n",
    "\n",
    "train = train.map(clean_training_sample, batched=False)\n",
    "test = test.map(clean_training_sample, batched=False)\n",
    "\n",
    "print(train[0])\n",
    "\n",
    "train = train.filter(lambda ds: ds['hardness'] == \"<s>Easy\" or ds['hardness'] == \"<s>Medium\")\n",
    "test = test.filter(lambda ds: ds['hardness'] == \"<s>Extra Hard\" or ds['hardness'] == \"<s>Extra Hard\")\n",
    "\n",
    "print(train[0])\n",
    "\n",
    "train = train.select(range(400)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"<s>### Instruction:\\nBar chart x axis product name y axis how many product name , rank by the Y-axis in desc .\\n### Input:\\n[('product_id', 'numeric'), ('product_type_code', 'categorical'), ('product_name', 'categorical'), ('product_price', 'categorical')]\\n###Thought Process:\\n1. **Understand the data**: Identify numerical and categorical columns from the input.\\n 2. **Determine the chart type**: Choose an appropriate mark (`bar`, `line`, `point`, `arc`) based on the instruction and data type.\\n 3. **Define encoding**: Assign x-axis, y-axis, aggregation function (if applicable), and any color mapping.\\n 4. **Identify transformations**: Determine whether filtering, binning, grouping, sorting, or top-k selection is required.\\n 5. **Flatten into a Vega-Zero specification**: Convert the reasoning into the keyword-based format required by Vega-Zero.\\n\\n### Response:\\nmark bar data products encoding x product_name y aggregate count product_name transform group x sort y desc\\n</s></s>\", 'hardness': '<s>Medium'}\n"
     ]
    }
   ],
   "source": [
    "print(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>### Instruction:\n",
      "Bar chart x axis product name y axis how many product name , rank by the Y-axis in desc .\n",
      "### Input:\n",
      "[('product_id', 'numeric'), ('product_type_code', 'categorical'), ('product_name', 'categorical'), ('product_price', 'categorical')]\n",
      "### Response:\n",
      "mark bar data products encoding x product_name y aggregate count product_name transform group x sort y desc\n",
      "</s></s>\n"
     ]
    }
   ],
   "source": [
    "print(train[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "e108ea2eaa74441584b7d97f8cb5b4b0",
      "c9d403a79efe440ea406df0f14b6bf3a",
      "760cd62a2f294669906b187d5e000014",
      "a68a8ab6cae14d31a45a3fcb0dfa360a",
      "1bfd12f68ae743619975e9ceb5555731",
      "99e9d9e65254495781e539c609b72d9a",
      "36e6cc28aad547bd946aeb8d39b79337",
      "e1a6d4eda937414fb164c8d423042dd6",
      "7c75d273e20746e18300976960f4edd4",
      "c757bae880ea46e3a38466f91e7ad978",
      "23169670cc0444b8a12d525fef80f445",
      "85c4fbc1751d4f35b6d81ea5278755cb",
      "58ad610d75fe497ba65de6f683bd0e8a",
      "294742b46534486a9ca3639c0c82bf5a",
      "0445a34071a94fd79e9b3a275aeb52a8",
      "bbec3814deb64a20ba516a4ce7e1af74",
      "cc6d3ba0ab094b73bd3434370f090541",
      "d22ac6cfb47f461d8998fde86bd74e03",
      "3e5714f3beba4983b3b9b94f756bf2b0",
      "4d5cc654034744fd95a4172f26e395ec",
      "faf8cb772bfd450ea58759c01f7639c9",
      "2505f5dfe8f745f9a13e34c64779431c",
      "a773d3278cb541649809e6f7649a4c1c",
      "a0c251b4d419418780232d9bf4f49d02",
      "1888fb4d5f5344428e14a917e57cdd26",
      "cdabdc345d9b492db6cbce0352de53ea",
      "b18fafbe62414ec691418946c27b448b",
      "ce57580b70ea442bada9525e80d84fee",
      "108bfc13e5084ca9ad7cafdff20b6d22",
      "a1555c2e942b43cc9c4941b06efb76f4",
      "f853a83267f94aa18765390a0ecb80f3",
      "f917768aaaf54d76b33aa96b9997d563",
      "07023caa7e494a13b8ea3853141b8089",
      "7b51d25886d740b2a8d259e091cffc51",
      "c2d69dbc2d2747c6bfae824ea80a6877",
      "befce5edb9ba418db491843ac5b921f4",
      "3bff36ea638c497b8badaf288a433186",
      "e8718f3d22f345bbbdc7198c208b9434",
      "e763ee0f53034737ab315124de0198ed",
      "6aad5767036d47928641666fc07b0f92",
      "d61e5317d07146be8b510da6546737e3",
      "9f2e0b28cc924db4b9c5ed7467c1e93a",
      "cf783c0a1a584212a0238de5d23a8b33",
      "fc557c3c7c1044fc9add53af9a54ab92",
      "101c4337217c42f7a8a00115e4b6076f",
      "d81a089b6a5a4baca816e85e1241b28e",
      "e68c84509fae4c5099d868ea9421c32a",
      "535405eff48b44b082b72267f0c3c03d",
      "a10ba1a1d89e4e53ad40e3d5ef763389",
      "1ba925fbf1344a42b04151a5f4ffc6b9",
      "f9f4d47e990f434aaa1d9e97665dcdcf",
      "3832410f12b945718d5c4b8125bc38ce",
      "46f3de843e0b4521b71907c7334d4d66",
      "e167447c37134eeea58cadedab3a35a4",
      "3ba8ae2512574c36b5b2e40da4259935",
      "5f16c6c4bf9a420c841afa5e73796750",
      "fb813be5851040a6a6128e5518708c69",
      "294d1b701cc84c0bac60cfdd4f689bd1",
      "bbd7f8bb1d7d4900a30c5012a719f7b3",
      "d18ee99d144c4422b90ca29b58e5ffd4",
      "a24d2a5732ea47178c551efd6e60699f",
      "4a458335c4854c049375ab5b0ef01e6e",
      "c6840e0cbd2d406d9a951a86a1ee1566",
      "fa6baf7521c94b37972ff1bac316eea0",
      "35a91bf3f3f744aca835c793000a62ce",
      "992d929340764f99ad129bb514742c9a",
      "42ca301f5a344713b7d487aa08100ed0",
      "29013e03099941c29edff7078c068755",
      "98b94c534ec844d1996fcb9df8d2b52f",
      "a67ba78885c8440cba2311363aaf36ae",
      "738e9b88f0654309b6c8f2b88a4cca64",
      "8de2c15200374452a8af14524d5d7441",
      "2bf0c972356349169a9add8e60b5203f",
      "91623e83138b4f369154fc70b403798e",
      "89b99b3d48634e0e8c3528f358ffb5a7",
      "add020a8e0724b64a096527d921204fd",
      "80e216e451894eb0bc846f16c2ed0514"
     ]
    },
    "editable": true,
    "id": "OJXpOgBFuSrc",
    "outputId": "591df792-8985-4786-d856-a1b37201f083",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d664406b1784058bf327cb97a968158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, config['bitsandbytes']['bnb_4bit_compute_dtype'])\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=config['bitsandbytes']['use_4bit'],\n",
    "    bnb_4bit_quant_type=config['bitsandbytes']['bnb_4bit_quant_type'],\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=config['bitsandbytes']['use_nested_quant'],\n",
    ")\n",
    "\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and config['bitsandbytes']['use_4bit']:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config['fine_tuning']['model']['ref_model_name'],\n",
    "    quantization_config=bnb_config,\n",
    "    device_map = {\"\": 0}\n",
    ")\n",
    "model.config.use_cache = config['fine_tuning']['model']['use_cache']\n",
    "model.config.pretraining_tp = config['fine_tuning']['model']['pretraining_tp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS Token: 128000\n",
      "EOS Token: 128001\n",
      "PAD Token: 128256\n",
      "['<|begin_of_text|>', '<|end_of_text|>', '[PAD]']\n",
      "Without special tokens:\n",
      "Tokenized Text: ['<|begin_of_text|>', '<s', '>', '###', ' Instruction', ':\\n', 'Bar', ' chart', ' x', ' axis', ' product', ' name', ' y', ' axis', ' how', ' many', ' product', ' name', ',', ' rank', ' by', ' the', ' Y', '-axis', ' in', ' desc', '.\\n', '###', ' Input', ':\\n', '[', \"('\", 'product', '_id', \"',\", \" '\", 'numeric', \"'),\", \" ('\", 'product', '_type', '_code', \"',\", \" '\", 'c', 'ategorical', \"'),\", \" ('\", 'product', '_name', \"',\", \" '\", 'c', 'ategorical', \"'),\", \" ('\", 'product', '_price', \"',\", \" '\", 'c', 'ategorical', \"')]\\n\", '###', ' Response', ':\\n', 'mark', ' bar', ' data', ' products', ' encoding', ' x', ' product', '_name', ' y', ' aggregate', ' count', ' product', '_name', ' transform', ' group', ' x', ' sort', ' y', ' desc', '\\n', '</', 's', '></', 's', '>']\n",
      "Token IDs: [128000, 45147, 29, 14711, 30151, 512, 3511, 9676, 865, 8183, 2027, 836, 379, 8183, 1268, 1690, 2027, 836, 1174, 7222, 555, 279, 816, 36421, 304, 6697, 16853, 14711, 5688, 512, 58, 493, 3107, 851, 518, 364, 20173, 4670, 4417, 3107, 1857, 4229, 518, 364, 66, 47147, 4670, 4417, 3107, 1292, 518, 364, 66, 47147, 4670, 4417, 3107, 9217, 518, 364, 66, 47147, 50724, 14711, 6075, 512, 4075, 3703, 828, 3956, 11418, 865, 2027, 1292, 379, 24069, 1797, 2027, 1292, 5276, 1912, 865, 3460, 379, 6697, 198, 524, 82, 1500, 82, 29]\n",
      "Token IDs: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config['fine_tuning']['model']['ref_model_name'],\n",
    "                                          trust_remote_code=True,\n",
    "                                          add_bos_token=False)#, add_eos_token=True, use_fast=True)\n",
    "tokenizer.add_special_tokens(config['fine_tuning']['tokenizer']['padding_token'])\n",
    "tokenizer.padding_side = config['fine_tuning']['tokenizer']['padding_side']\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "assert model.config.pad_token_id == tokenizer.pad_token_id, \"The model's pad token ID does not match the tokenizer's pad token ID!\"\n",
    "\n",
    "print(\"BOS Token:\", tokenizer.bos_token_id)\n",
    "print(\"EOS Token:\", tokenizer.eos_token_id)\n",
    "print(\"PAD Token:\", tokenizer.pad_token_id)\n",
    "print(tokenizer.all_special_tokens)\n",
    "sample_sentence = train['text'][0]\n",
    "\n",
    "# Tokenize without special tokens\n",
    "tokenized_output_no_special = tokenizer(sample_sentence, add_special_tokens=True)\n",
    "print(\"Without special tokens:\")\n",
    "print(\"Tokenized Text:\", [tokenizer.decode([x]) for x in tokenized_output_no_special[\"input_ids\"]])\n",
    "print(\"Token IDs:\", tokenized_output_no_special[\"input_ids\"])\n",
    "print(\"Token IDs:\", tokenized_output_no_special[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true,
    "id": "FkkT0skezLoG",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/opt/conda/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46759247926c42259c36fa5e23e03fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae3f813fb3145c6a897efa1f5512468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/221 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.090200</td>\n",
       "      <td>1.107151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.999200</td>\n",
       "      <td>0.999170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./models/larger-tokenizer/tokenizer_config.json',\n",
       " './models/larger-tokenizer/special_tokens_map.json',\n",
       " './models/larger-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=config['lora']['lora_alpha'],\n",
    "    lora_dropout=config['lora']['lora_dropout'],\n",
    "    r=config['lora']['lora_r'],\n",
    "    bias=config['lora']['bias'],\n",
    "    task_type=config['lora']['task_type'],\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=config['fine_tuning']['model']['output_dir'],\n",
    "    num_train_epochs= config['fine_tuning']['hyperparameters']['num_train_epochs'],\n",
    "    per_device_train_batch_size= config['fine_tuning']['hyperparameters']['per_device_train_batch_size'] ,\n",
    "    gradient_accumulation_steps= config['fine_tuning']['hyperparameters']['gradient_accumulation_steps'] ,\n",
    "    optim=config['fine_tuning']['hyperparameters']['optim'],\n",
    "    save_steps=config['fine_tuning']['hyperparameters']['save_steps'],\n",
    "    logging_steps=config['fine_tuning']['hyperparameters']['logging_steps'],\n",
    "    learning_rate=config['fine_tuning']['hyperparameters']['learning_rate'],\n",
    "    weight_decay=config['fine_tuning']['hyperparameters']['weight_decay'],\n",
    "    fp16=config['fine_tuning']['hyperparameters']['fp16'],\n",
    "    bf16=config['fine_tuning']['hyperparameters']['bf16'],\n",
    "    max_grad_norm=config['fine_tuning']['hyperparameters']['max_grad_norm'],\n",
    "    max_steps=config['fine_tuning']['hyperparameters']['max_steps'],\n",
    "    warmup_ratio=config['fine_tuning']['hyperparameters']['warmup_ratio'],\n",
    "    group_by_length=config['fine_tuning']['hyperparameters']['group_by_length'],\n",
    "    lr_scheduler_type=config['fine_tuning']['hyperparameters']['lr_scheduler_type'],\n",
    "    report_to=config['fine_tuning']['hyperparameters']['report_to'],\n",
    "    load_best_model_at_end = True,\n",
    "    evaluation_strategy = IntervalStrategy.STEPS,\n",
    "    eval_steps = 50,\n",
    "    metric_for_best_model='eval_loss'\n",
    ")\n",
    "\n",
    "print('1')\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train,\n",
    "    eval_dataset = test,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=200,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=config['fine_tuning']['sft']['packing'],\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "print('2')\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "trainer.model.save_pretrained(config['fine_tuning']['model']['output_dir'])\n",
    "trainer.tokenizer.save_pretrained(config['fine_tuning']['tokenizer']['output_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "#dataset_location= config['hugginfaces']['dataset']['name']\n",
    "#test = load_dataset(config['hugginfaces']['dataset']['name'],\n",
    "                    #split=\"test\", revision=\"4.0.4\", cache_dir = \"./tmp\")\n",
    "pipe = pipeline(task=\"text-generation\", model=trainer.model, tokenizer=trainer.tokenizer, max_length=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Push to the hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af643735fb643b699d444f558d429c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config['fine_tuning']['model']['ref_model_name'],\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config['fine_tuning']['tokenizer']['output_dir'], trust_remote_code=True)\n",
    "\n",
    "tokenizer.padding_side = config['fine_tuning']['tokenizer']['padding_side']\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, config['fine_tuning']['model']['output_dir'])\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS Token: 128000\n",
      "EOS Token: 128001\n",
      "PAD Token: 128256\n",
      "['<|begin_of_text|>', '<|end_of_text|>', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "# quando si modifica il modello si deve successivamente rifare il resize\n",
    "\n",
    "\n",
    "print(\"BOS Token:\", tokenizer.bos_token_id)\n",
    "print(\"EOS Token:\", tokenizer.eos_token_id)\n",
    "print(\"PAD Token:\", tokenizer.pad_token_id)\n",
    "print(tokenizer.all_special_tokens)\n",
    "assert model.config.pad_token_id == tokenizer.pad_token_id, \"The model's pad token ID does not match the tokenizer's pad token ID!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/200-easy-full-tokenizer/tokenizer_config.json',\n",
       " './models/200-easy-full-tokenizer/special_tokens_map.json',\n",
       " './models/200-easy-full-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./models/200-easy-full\", save_embedding_layers=True)\n",
    "tokenizer.save_pretrained(\"./models/200-easy-full-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cefbd07788eb467195d0bea073108750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./models/200-easy-full\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./models/200-easy-full-tokenizer\", trust_remote_code=True)\n",
    "\n",
    "tokenizer.padding_side = config['fine_tuning']['tokenizer']['padding_side']\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# login(token=os.getenv('hf_write_token'))\n",
    "# model.push_to_hub(\"DeepvizLab/newton-7b-full\")\n",
    "# tokenizer.push_to_hub(\"DeepvizLab/newton-7b-full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since LucaPodo/newton-dataset-v1 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at tmp/LucaPodo___newton-dataset-v1/default/0.0.0/4bbe690926a2757f8f60bdff61f63b9558fda168 (last modified on Thu Feb 20 23:26:11 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Find the number of trains starting from each origin Plot them as bar chart , and order in asc by the y-axis .\n",
      "### Input:\n",
      "[('id', 'numeric'), ('train_number', 'numeric'), ('name', 'categorical'), ('origin', 'categorical'), ('destination', 'categorical'), ('time', 'categorical'), ('interval', 'categorical')]\n",
      "### Response:\n",
      "mark\n",
      "\n",
      "mark bar data train encoding x origin y aggregate count origin transform group x sort y asc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_location= \"LucaPodo/newton-dataset-v1\"\n",
    "test = load_dataset(dataset_location, split=\"test\", revision=\"4.0.4\", cache_dir = \"./tmp\")\n",
    "pipe = pipeline(task=\"text-generation\", model=base_model, tokenizer=tokenizer, max_length=400, eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "i = 100\n",
    "prompt = test[i]['text'].split(\"### Response:\")[0] + \"### Response:\\nmark\"\n",
    "prompt = \"### Instruction:\" + prompt.split('### Instruction:')[1]\n",
    "print(prompt + '\\n')\n",
    "groundtruth = test[i]['text'].split(\"### Response:\")[1].strip()\n",
    "print(groundtruth + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Find the number of trains starting from each origin Plot them as bar chart , and order in asc by the y-axis .\n",
      "### Input:\n",
      "[('id', 'numeric'), ('train_number', 'numeric'), ('name', 'categorical'), ('origin', 'categorical'), ('destination', 'categorical'), ('time', 'categorical'), ('interval', 'categorical')]\n",
      "### Response:\n",
      "mark bar data trains encoding x origin y aggregate count origin transform group x sort y asc\n",
      "### Min Score:\n",
      "0.95\n"
     ]
    }
   ],
   "source": [
    "result = pipe(prompt)\n",
    "print(result[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "lcc_arn": "arn:aws:sagemaker:eu-north-1:490853539869:studio-lifecycle-config/my-studio-lcc"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
