{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 20476,
     "status": "ok",
     "timestamp": 1697488125462,
     "user": {
      "displayName": "Luca Podo",
      "userId": "02050102078660879808"
     },
     "user_tz": -120
    },
    "id": "ZY8Y-mItgSyv",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "ac3c43e6-9316-4719-d533-9c1dfc273686",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "astropy 6.0.1 requires numpy<2,>=1.22, but you have numpy 2.0.0 which is incompatible.\n",
      "numba 0.59.1 requires numpy<1.27,>=1.22, but you have numpy 2.0.0 which is incompatible.\n",
      "sagemaker 2.215.0 requires numpy<2.0,>=1.9.0, but you have numpy 2.0.0 which is incompatible.\n",
      "sagemaker 2.215.0 requires protobuf<5.0,>=3.12, but you have protobuf 5.27.1 which is incompatible.\n",
      "sagemaker-datawrangler 0.4.3 requires sagemaker-data-insights==0.4.0, but you have sagemaker-data-insights 0.3.3 which is incompatible.\n",
      "sphinx 7.2.6 requires docutils<0.21,>=0.18.1, but you have docutils 0.16 which is incompatible.\n",
      "statsmodels 0.14.1 requires numpy<2,>=1.18, but you have numpy 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 wandb torch==2.0.1 python-dotenv pyyaml --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "astropy 6.0.1 requires numpy<2,>=1.22, but you have numpy 2.0.0 which is incompatible.\n",
      "numba 0.59.1 requires numpy<1.27,>=1.22, but you have numpy 2.0.0 which is incompatible.\n",
      "pyarrow 15.0.2 requires numpy<2,>=1.16.6, but you have numpy 2.0.0 which is incompatible.\n",
      "sagemaker 2.215.0 requires numpy<2.0,>=1.9.0, but you have numpy 2.0.0 which is incompatible.\n",
      "sagemaker 2.215.0 requires protobuf<5.0,>=3.12, but you have protobuf 5.27.1 which is incompatible.\n",
      "sagemaker-datawrangler 0.4.3 requires sagemaker-data-insights==0.4.0, but you have sagemaker-data-insights 0.3.3 which is incompatible.\n",
      "sphinx 7.2.6 requires docutils<0.21,>=0.18.1, but you have docutils 0.16 which is incompatible.\n",
      "statsmodels 0.14.1 requires numpy<2,>=1.18, but you have numpy 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade  --force-reinstall -q git+https://github.com/lucapodo/pip-newton.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26.4\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.0\n",
      "    Uninstalling numpy-2.0.0:\n",
      "      Successfully uninstalled numpy-2.0.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker 2.215.0 requires protobuf<5.0,>=3.12, but you have protobuf 5.27.1 which is incompatible.\n",
      "sagemaker-datawrangler 0.4.3 requires sagemaker-data-insights==0.4.0, but you have sagemaker-data-insights 0.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    AutoModelForSeq2SeqLM\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, TaskType\n",
    "from trl import SFTTrainer\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, set_seed, create_reference_model\n",
    "from trl.core import LengthSampler\n",
    "import pandas as pd\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "from newtonmetrics.vegazero.VegaZero2VegaLite import VegaZero2VegaLite \n",
    "from newtonmetrics.newton.newton import Newton \n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "load_dotenv(\"config.env\")\n",
    "\n",
    "with open('config.yaml') as file:\n",
    "    config_ = yaml.safe_load(file)\n",
    "\n",
    "n = Newton()\n",
    "n.NormalizeData(2)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.0.1+cu117\n",
      "CUDA is not available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_dataset(tokenizer):\n",
    "    ds = load_dataset(dataset_name, split=\"test\", revision=\"0.0.2\",download_mode=\"force_redownload\", cache_dir=\"./tmp\" )\n",
    "    def remove_reponse(sample):\n",
    "        sample['hardness'] = sample['text'].split(\"@\")[0].strip()\n",
    "        text = sample['text'].split('@')[1]\n",
    "        \n",
    "        sample['response'] = text.split(\"### Response:\")[1].strip()\n",
    "        sample['text'] = text.split(\"### Response:\")[0] + \"### Response: mark\\n\"\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(remove_reponse, batched=False)\n",
    "    \n",
    "    # Split the dataset into the first 100 instances and the rest\n",
    "    # first_100_instances = ds.select(range(100))\n",
    "    # remaining_instances = ds.select(range(100, len(ds)))\n",
    "\n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(sample[\"text\"], padding = True)\n",
    "        #sample[\"query\"] = '<s>' + tokenizer.decode(sample[\"input_ids\"]).split('<s>')[2]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds\n",
    "\n",
    "\n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "\n",
    "def getDataframe_old(text):\n",
    "    instruction = text.split(\"<</DATA>>\")[1].split('[/INST]')[0].strip()\n",
    "    print(instruction)\n",
    "    result = newton_test_df[newton_test_df['instruction'].str.contains(instruction, case=False, na=False)]\n",
    "    id = result['tvBench_id'].values[0]\n",
    "    tmp_df =  pd.read_csv(os.path.join('/content/drive/MyDrive/DeepvizLab/NewtonLLM/data/datasets', id+'.csv'), index_col = 0)\n",
    "    return tmp_df\n",
    "\n",
    "def can_compile(result, ground):\n",
    "    res = n.can_compile(result,ground)\n",
    "    return res\n",
    "\n",
    "def get_newton_score(result, ground):\n",
    "    # res = n.compute_score_raff(result, ground)\n",
    "    res = n.compute_score_100(result,ground)\n",
    "    # res = n.only_jaacard(result,ground)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2788,
     "status": "ok",
     "timestamp": 1697488139905,
     "user": {
      "displayName": "Luca Podo",
      "userId": "02050102078660879808"
     },
     "user_tz": -120
    },
    "id": "TyQrFGl5gxpR",
    "outputId": "a9513170-dce9-438c-b1b4-421f72652818",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlpodo\u001b[0m (\u001b[33mdeepvizlab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#login(token=\"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\")\n",
    "login(token=\"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\")\n",
    "os.environ[\"WANDB_API_KEY\"] = \"e487be984a9468fa0fefabb74883fd81beed289f\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1697488139905,
     "user": {
      "displayName": "Luca Podo",
      "userId": "02050102078660879808"
     },
     "user_tz": -120
    },
    "id": "XCUOWlaDuUVh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### run again\n",
    "dataset_name = \"LucaPodo/newton-dataset-v1\"\n",
    "dataset_version = \"0.02\"\n",
    "model_name = \"./models/200-easy\"\n",
    "# model_name = \"DeepvizLab/newton-7b-1k\"\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "lora_r = 128 # 64\n",
    "lora_alpha = 16 # 16\n",
    "lora_dropout = 0.1\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype = \"bfloat16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_nested_quant = False\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "output_dir = \"/content/drive/MyDrive/DeepvizLab/NewtonLLM/llama2\"\n",
    "num_train_epochs = 1\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "per_device_train_batch_size = 4\n",
    "per_device_eval_batch_size = 4\n",
    "gradient_accumulation_steps = 1\n",
    "gradient_checkpointing = True\n",
    "max_grad_norm = 0.3\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.001\n",
    "optim = \"paged_adamw_32bit\"\n",
    "lr_scheduler_type = \"cosine\"\n",
    "max_steps = -1\n",
    "warmup_ratio = 0.03\n",
    "group_by_length = True\n",
    "save_steps = 0\n",
    "logging_steps = 25\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "max_seq_length = None\n",
    "packing = False\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1697489141760,
     "user": {
      "displayName": "Luca Podo",
      "userId": "02050102078660879808"
     },
     "user_tz": -120
    },
    "id": "8Pj3HN76n0qc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: No names found, cannot describe anything.\n"
     ]
    }
   ],
   "source": [
    "config = PPOConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=learning_rate,#1.41e-5,\n",
    "    mini_batch_size=1,\n",
    "    batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    early_stopping=True,\n",
    "    target_kl=0.8,\n",
    "    kl_penalty=\"kl\",\n",
    "    seed=0,\n",
    "    log_with='wandb',\n",
    "    #callbacks=[WandbCallback(log=False)]\n",
    ")\n",
    "\n",
    "# learning_rate=1e-5,  # Lowered from 2e-5 to make updates more conservative\n",
    "# mini_batch_size=4,   # Increased from 1\n",
    "# batch_size=4,        # Increased from 1\n",
    "# gradient_accumulation_steps=1,\n",
    "# early_stopping=True,\n",
    "# target_kl=0.8,       # Reduced to keep policy updates more conservative\n",
    "\n",
    "set_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310,
     "referenced_widgets": [
      "b26b30a1023b4e8dae29c1eae7e73591",
      "3d3130efc1204d309d1991e074cd33ac",
      "96eace3e3f0d462ab9bbf8bbb9567d8c",
      "a8aba141ce7b494b9bc45b323ae9a1f0",
      "2489079e7d804a59872806474057f8ff",
      "8fe94d49732e43c38b5ac9c8220db60a",
      "47fc2dfcf0404739ae20a16898806d1f",
      "474464b96cb3496cbff29c4f268b01b4",
      "2d7e10422e844a73b5f049bbd7641819",
      "19e817c6b7c44febaefadb341b25ac3d",
      "00af8d605784400fa37cd756f176969c",
      "45754da9db324829a911dfc020835d55",
      "ae4c4c0ef813471c8d5dc8b504b7be41",
      "e8d357335eee41929c442543cd43f48a",
      "ac5d361138dd403fb5277e4e7b8a7b42",
      "fca26bb9417b431585e057625692a380",
      "18e293608f794ef8bb9388002f5c7f2a",
      "bfc81d05f5524ebe8f064a5c2cad95fd",
      "e195ad4b5e3948889d3526eb91674395",
      "293eb0a5ef4140dfb1a91c4a2a0dc58a",
      "e8af314fafe542d4970aa6ec14e2300f",
      "6a460670f8ae433bb83305868be9c7a5",
      "e2365bd75cfa46788ebabcc6c34387e8",
      "2068697c9d1d4f49b7e868d416f11552",
      "836b6e1ae0c846f8a2df7ef7e19392e8",
      "548a2af3c354404c84dcb1f4a9c0c4d2",
      "bbd12f6f5354452997b8eb5877fb0b14",
      "45544ba030fc4e1681659a7970b8cc69",
      "82b798a9ba3647c08e128f06c95a18ce",
      "5d840dd6ee5d4b1aa5e23a88b3443191",
      "2c78a90903f643dc841d227d43d9b48c",
      "e19e1172135849d393305fc290303add",
      "8f579d8838cb453f8505be353f8d8a5f",
      "ab1895dfc9514cdcbff29e3cc1a9ec05",
      "dea5795197df4f8a82622d59b6929f4f",
      "1372db3612c04090a03b1ec43176db2d",
      "2f4fd2d47f734bdbaec6d9ba139f7237",
      "eb8dadf9c53c481194eea65260cc27cb",
      "ebfdbdb4a89445f0a210fed670857d03",
      "27d50d9ab1a648488f7197007d584e1e",
      "ea9f3e06e9c641329b37f800f259188a",
      "d2d0a26534944bde82b41885f34f542d",
      "be3222b051eb45b085b8936628d053b5",
      "af73781cbc824723b6d709cda70a8e66",
      "594c4d6f49494b00a4e1d0ff3e1632ad",
      "fd8db49031c54560864bb01c5c165ab8",
      "51c0d012361a495ab2aea802c0a10877",
      "f5d2fd1c803048c688fee1b0b877e18f",
      "74283ab3d05f4c28912a697ceb7e8b3d",
      "42004c72491e406a83772a13ec6f6704",
      "635cf1e424fa413081d54409d993a504",
      "d6a8a774de124549ab23484fb10fa780",
      "1e3e82be2a714d2aa1ed53f2cea45238",
      "54afbbedd0f54bd190e0b2a175504eb7",
      "4992e6a7645c4f588e71a9cfcff8c953",
      "845230a6791845858a93e97154a055c4",
      "c400b339866b4a1bb59d4d88b1407d13",
      "55ba7c7ad9bd4af5ac920a08837d8172",
      "22f0a3a5aa9249c7b55a5b684ddc6642",
      "3c0d204eb524423a8db7a31ba00ac3d2",
      "4007135e2d464e44b26affb6687c0b31",
      "04c545a4566249608eaf932352a87946",
      "f6c7480dd6c94e719b13d37218e14cf6",
      "326eda82d2e94786954b09a8a39a9a4d",
      "4e37a2bae7d44275bb54b7a0f2f2a718",
      "9bb8e988d47e44de883963e77a957ae4",
      "2c8893f96e0f4c0ba2df0be8e3856c62",
      "36e7b978f57e4f0a955373f4ca3096aa",
      "610acc82ab004ae986ba36d6249c502c",
      "0e1fd873d4e343438548ae95ee7b6bcc",
      "305915c55ba44397a509c43ae6b2f174",
      "a86e28de3ada4a09b2c5884f14318f69",
      "6238132ad33b4c0d9ca27d06140a465a",
      "5de0df0334fe4cd7b0960d01b9f8efd8",
      "f916c9cc974e4cfeab2dba3d3cbe688a",
      "ebc378b5b14943b29baa9bada9ad0e1e",
      "23a31ebfc9764dbea7ac5621fe4c48ca"
     ]
    },
    "executionInfo": {
     "elapsed": 1025306,
     "status": "ok",
     "timestamp": 1697490168108,
     "user": {
      "displayName": "Luca Podo",
      "userId": "02050102078660879808"
     },
     "user_tz": -120
    },
    "id": "XS7SK7y6H0BO",
    "outputId": "5a5ae91b-1d41-46e3-9e36-cea642121a45",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed09f843c0ce49abb99a39ea026b0ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Load the model in 4-bit quantization\n",
    "#compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",#torch.bfloat16,#\"float16\"#torch.bfloat16,#\"float16\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load base model\n",
    "model_ = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    torch_dtype=torch.bfloat16,#torch.bfloat16,\n",
    "    cache_dir=\"./tmp\"\n",
    ")\n",
    "model_.config.use_cache = False\n",
    "model_.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer_ = AutoTokenizer.from_pretrained(\"./models/200-easy-tokenizer\", trust_remote_code=True, cache_dir=\"./tmp\") #nel caso rimuovere torch_dtype torch.bfloat16\n",
    "model_.resize_token_embeddings(len(tokenizer_))\n",
    "model_.config.pad_token_id = tokenizer_.pad_token_id\n",
    "\n",
    "assert model_.config.pad_token_id == tokenizer_.pad_token_id, \"The model's pad token ID does not match the tokenizer's pad token ID!\"\n",
    "\n",
    "tokenizer_.padding_side = \"left\"\n",
    "print(tokenizer_.all_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8547,
     "status": "ok",
     "timestamp": 1697490199386,
     "user": {
      "displayName": "Luca Podo",
      "userId": "02050102078660879808"
     },
     "user_tz": -120
    },
    "id": "KgnyLEo4gDCa",
    "outputId": "6c0fd9bd-cfeb-4a5d-df29-63e0d3522af5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "current_device = Accelerator().local_process_index\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    model_,\n",
    "    #torch_dtype=torch.bfloat16, #torch.bfloat16,\n",
    "    device_map={\"\": current_device},\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = build_dataset(tokenizer_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset.filter(lambda ds: ds['hardness'] == \"Easy\" or ds['hardness'] == \"Medium\")\n",
    "test = dataset.filter(lambda ds: ds['hardness'] == \"Extra Hard\" or ds['hardness'] == \"Extra Hard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1697491756575,
     "user": {
      "displayName": "Luca Podo",
      "userId": "02050102078660879808"
     },
     "user_tz": -120
    },
    "id": "hag_CSmWzKHn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_ref = create_reference_model(ppo_model)\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config,\n",
    "    ppo_model,\n",
    "    ref_model=model_ref,\n",
    "    tokenizer=tokenizer_,\n",
    "    dataset=train,\n",
    "    data_collator=collator,\n",
    "    optimizer=None,\n",
    ")\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer_.pad_token_id,\n",
    "    \"eos_token_id\": tokenizer_.eos_token_id,\n",
    "}\n",
    "\n",
    "output_min_length = 100\n",
    "output_max_length = 400\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# self-made models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accard_similarity(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(set(list1)) + len(set(list2))) - intersection\n",
    "    return float(intersection) / union# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_observer_loss_with_differences(predicted_reward, discrete_reward, current_stats, previous_stats, weights):\n",
    "    \"\"\"\n",
    "    Calculate the observer model's loss based on the differences in statistics\n",
    "    between the current and previous steps.\n",
    "    \n",
    "    Parameters:\n",
    "    - current_stats: dict, containing the current statistics from the PPO learner.\n",
    "    - previous_stats: dict, containing the statistics from the previous step.\n",
    "    - weights: dict, containing weights for each component of the loss.\n",
    "    \n",
    "    Returns:\n",
    "    - loss: float, the calculated loss for the observer model focusing on changes in performance.\n",
    "    \"\"\"\n",
    "    # Calculate differences in statistics\n",
    "    reward_diff = abs(predicted_reward - discrete_reward)\n",
    "    policy_loss_diff = current_stats.get('ppo/loss/policy')[0] - previous_stats.get('ppo/loss/policy')[0]\n",
    "    value_loss_diff = current_stats.get('ppo/loss/value')[0] - previous_stats.get('ppo/loss/value')[0]\n",
    "    kl_divergence_diff = current_stats.get('objective/kl') - previous_stats.get('objective/kl')\n",
    "    entropy_diff = current_stats.get('objective/entropy') - previous_stats.get('objective/entropy')\n",
    "    \n",
    "#     print(f\"current policy: {current_stats.get('ppo/loss/policy')[0]}, previous policy: {previous_stats.get('ppo/loss/policy')[0]}, current value: {current_stats.get('ppo/loss/value')[0]}, entropy diff: {entropy_diff}, rewrad diff: {reward_diff}\")\n",
    "\n",
    "#     print(f\"policy diff: {policy_loss_diff}, val diff: {value_loss_diff}, kl diff: {kl_divergence_diff}, entropy diff: {entropy_diff}, rewrad diff: {reward_diff}\")\n",
    "    \n",
    "    # Calculate components of the loss based on these differences\n",
    "    policy_loss_component = weights['alpha'] * policy_loss_diff\n",
    "    value_loss_component = weights['beta'] * value_loss_diff\n",
    "    kl_divergence_component = weights['gamma'] * kl_divergence_diff\n",
    "    entropy_component = weights['delta'] * entropy_diff\n",
    "    \n",
    "    # Combine components to form the total loss\n",
    "    metrics = policy_loss_component + value_loss_component\n",
    "    \n",
    "    # print(f\"metrics: {metrics}\")\n",
    "    \n",
    "    reward_benefit =  metrics / (reward_diff + weights['epsolon'])\n",
    "    \n",
    "    print(f\"metrics: {reward_benefit}\")\n",
    "    \n",
    "    loss = weights['alpha'] * abs(reward_diff) + weights['beta'] * reward_benefit\n",
    "    \n",
    "    # trial_loss = weights['alpha'] * reward_diff\n",
    "    x = torch.tensor([loss], requires_grad=True)\n",
    "    print(f\"loss: {x}\")\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "weights = {\n",
    "    'pl': -1.0,  # Negative weight if we want to encourage reduction in policy loss\n",
    "    'vl': -1.0,   # Negative weight if we want to encourage reduction in value loss\n",
    "    'kld': 0.1,  # Positive or negative based on whether you want to increase or decrease KL divergence\n",
    "    'en': -0.1,  # Negative if reducing entropy is desired\n",
    "    'alpha': 1.0,\n",
    "    'beta': 0.5,\n",
    "    'epsolon': 0.01\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DualEncoderSelfAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim=1):\n",
    "        super(DualEncoderSelfAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
    "        \n",
    "        # Self-Attention Layers\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Enhanced Feedforward Layers after concatenation\n",
    "        self.ffnn1 = nn.Linear(2 * embed_dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.1)  # Adjust dropout rate as needed\n",
    "        \n",
    "        self.ffnn2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.1)  # Adjust dropout rate as needed\n",
    "\n",
    "        self.ffnn3 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        # Output layer to produce a probability between 0 and 1\n",
    "        self.output = nn.Linear(hidden_dim // 2, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Weight initialization\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                if layer.bias is not None:\n",
    "                    nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "    def encode(self, x):\n",
    "        # Embedding layer\n",
    "        embedded_x = self.embedding(x.long())\n",
    "        \n",
    "        # Self-attention mechanism\n",
    "        Q = self.query(embedded_x)\n",
    "        K = self.key(embedded_x)\n",
    "        V = self.value(embedded_x)\n",
    "        \n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (K.size(-1) ** 0.5)\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        attention_output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # Pool the output of the attention to form a single vector per sentence\n",
    "        pooled_output = attention_output.mean(dim=1)\n",
    "        return pooled_output\n",
    "    \n",
    "    def forward(self, sentence_a, sentence_b):\n",
    "        # Encode both sentences separately\n",
    "        encoded_a = self.encode(sentence_a)\n",
    "        encoded_b = self.encode(sentence_b)\n",
    "\n",
    "        # Concatenate the encoded outputs\n",
    "        concatenated = torch.cat((encoded_a, encoded_b), dim=-1)\n",
    "\n",
    "        # Passing through FFNN layers\n",
    "        ffnn_output = self.relu1(self.ffnn1(concatenated))\n",
    "        ffnn_output = self.dropout1(ffnn_output)\n",
    "        ffnn_output = self.relu2(self.ffnn2(ffnn_output))\n",
    "        ffnn_output = self.dropout2(ffnn_output)\n",
    "        ffnn_output = self.relu3(self.ffnn3(ffnn_output))\n",
    "\n",
    "        # Sigmoid output for final layer\n",
    "        output = self.sigmoid(self.output(ffnn_output))\n",
    "\n",
    "        # Return the output directly\n",
    "        return output.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaLR:\n",
    "    def __init__(self, n_epochs: int, decay_start_epoch: int) -> None:\n",
    "        \"\"\"\n",
    "        Linearly decay the leraning rate to 0, starting from `decay_start_epoch`\n",
    "        to the final epoch.\n",
    "\n",
    "        In practice\n",
    "\n",
    "        :param n_epochs: total number of epochs\n",
    "        :param decay_start_epoch: epoch in which the learning rate starts to decay\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            n_epochs - decay_start_epoch\n",
    "        ) > 0, \"Decay must start before the training session ends!\"\n",
    "        self.n_epochs = n_epochs\n",
    "        self.decay_start_epoch = decay_start_epoch\n",
    "\n",
    "    def step(self, epoch: int) -> float:\n",
    "        \"\"\"\n",
    "        One step of lr decay:\n",
    "        - if `epoch < self.decay_start_epoch` it doesn't change the learning rate.\n",
    "        - Otherwise, it linearly decay the lr to reach zero\n",
    "\n",
    "        :param epoch: current epoch\n",
    "        :returns: learning rate multiplicative factor\n",
    "        \"\"\"\n",
    "        return 1.0 - max(0, epoch - self.decay_start_epoch) / (\n",
    "            self.n_epochs - self.decay_start_epoch\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize observer model\n",
    "input_size = 100  \n",
    "hidden_sizes = [128, 64]  \n",
    "output_size = 1  \n",
    "observer_self_attn = DualEncoderSelfAttention(vocab_size=32001, embed_dim=128, hidden_dim=256, output_dim=1)\n",
    "# observer_model = SimpleFFNN(input_size, hidden_sizes, output_size)\n",
    "\n",
    "# Initialize optimizer for observer model\n",
    "optimizer_self_attn = torch.optim.Adam(observer_self_attn.parameters(), lr=0.005)\n",
    "# optimizer = torch.optim.Adam(observer_model.parameters(), lr=0.001)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "#             optimizer_self_attn,\n",
    "#             lr_lambda=LambdaLR(1500, 250).step,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Bert training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_loss(predicted_reward, discrete_reward, policy_loss, value_loss):\n",
    "    \"\"\"\n",
    "    Calculate the observer model's loss based on the differences in statistics\n",
    "    between the current and previous steps.\n",
    "    \n",
    "    Parameters:\n",
    "    - current_stats: dict, containing the current statistics from the PPO learner.\n",
    "    - previous_stats: dict, containing the statistics from the previous step.\n",
    "    - weights: dict, containing weights for each component of the loss.\n",
    "    \n",
    "    Returns:\n",
    "    - loss: float, the calculated loss for the observer model focusing on changes in performance.\n",
    "    \"\"\"\n",
    "    weights = {\n",
    "        'pl': -1.0,  # Negative weight if we want to encourage reduction in policy loss\n",
    "        'vl': -1.0,   # Negative weight if we want to encourage reduction in value loss\n",
    "        'kld': 0.1,  # Positive or negative based on whether you want to increase or decrease KL divergence\n",
    "        'en': -0.1,  # Negative if reducing entropy is desired\n",
    "        'alpha': 1.5,\n",
    "        'beta': 0.5,\n",
    "        'epsolon': 0.01\n",
    "    }\n",
    "    # print(predicted_reward)\n",
    "    # print(discrete_reward)\n",
    "    # predicted_reward = int(predicted_reward)\n",
    "    # discrete_reward = int(discrete_reward)\n",
    "    # Calculate components of the loss based on these differences\n",
    "    reward_diff = abs(predicted_reward - discrete_reward)\n",
    "\n",
    "    policy_loss_component = weights['alpha'] * policy_loss\n",
    "    value_loss_component = weights['beta'] * value_loss\n",
    "\n",
    "    metrics = policy_loss_component + value_loss_component\n",
    "    \n",
    "    \n",
    "    reward_benefit =  metrics / (reward_diff + weights['epsolon'])\n",
    "    \n",
    "    # print(f\"metrics: {reward_benefit}\")\n",
    "    \n",
    "    loss = weights['alpha'] * reward_diff + weights['beta'] * reward_benefit\n",
    "    # trial_loss = weights['alpha'] * reward_diff\n",
    "    print(f\"loss: {loss}\")\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize storage for dataset\n",
    "training_data = {\n",
    "    \"prediction\": [],\n",
    "    \"groundtruth\": [],\n",
    "    \"reward\": [],\n",
    "    \"policy_loss\":[],\n",
    "    \"value_loss\":[]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AdamW\n",
    "\n",
    "# Load pre-trained TinyBERT and its tokenizer\n",
    "model_name = 'prajjwal1/bert-tiny'  # Example TinyBERT model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)  # Assuming a regression task\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer_bert = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load data\n",
    "df_train = pd.read_csv(\"./training_data_easy.csv\")\n",
    "\n",
    "# Convert DataFrame to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df_train)\n",
    "\n",
    "# Tokenizer\n",
    "model_name = \"prajjwal1/bert-tiny\"  # or \"distilroberta-base\" if you switch to it\n",
    "tokenizer_1 = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Ensure the tokenizer uses the correct padding token\n",
    "if tokenizer_1.pad_token is None:\n",
    "    tokenizer_1.pad_token = tokenizer_1.eos_token\n",
    "    tokenizer_1.model.config.pad_token_id = tokenizer_1.eos_token_id\n",
    "\n",
    "# Formatting function adapted for your data structure\n",
    "def formatting_func(examples):\n",
    "    # combined = examples['prediction'] + ' ' + examples['groundtruth']\n",
    "    encoded = tokenizer_1(examples['prediction'], examples['groundtruth'],\n",
    "                            padding=\"max_length\",  # Ensure all outputs are padded to the max_length\n",
    "                            truncation=True,       # Ensure all outputs are truncated to fit the max_length\n",
    "                            max_length=201,        # Define max_length according to model's capability\n",
    "                            return_tensors=\"pt\")   # Use numpy to ensure compatibility with Hugging Face Dataset\n",
    "    \n",
    "    # print(\"Shape of input_ids:\", encoded['input_ids'].shape)\n",
    "    # print(\"Shape of attention_mask:\", encoded['attention_mask'].shape)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": encoded[\"input_ids\"],  # Remove batch dimension\n",
    "        \"attention_mask\": encoded[\"attention_mask\"],\n",
    "        \"reward\": examples[\"reward\"],\n",
    "        \"policy_loss\": examples['policy_loss'],\n",
    "        \"value_loss\": examples['value_loss']\n",
    "    }\n",
    "\n",
    "# Apply formatting function\n",
    "formatted_dataset = dataset.map(formatting_func, batched=False)  # `batched=True` for efficiency\n",
    "for i in range(5):  # Check the first few records\n",
    "    print(type(formatted_dataset[i]['input_ids']), formatted_dataset[i]['input_ids'].shape)\n",
    "    print(type(formatted_dataset[i]['attention_mask']), formatted_dataset[i]['attention_mask'].shape)\n",
    "# Split the dataset\n",
    "train_test_split = formatted_dataset.train_test_split(test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# from transformers import BertTokenizer, BertModel, BertConfig, BertForSequenceClassification\n",
    "# from transformers import AutoConfig, AutoModel\n",
    "\n",
    "# class SON_BERT(BertForSequenceClassification):\n",
    "#     # config = AutoConfig.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "    \n",
    "#     def __init__(self, config):\n",
    "#         super(SON_BERT, self).__init__(config)\n",
    "#         # Load model body\n",
    "#         self.bert = AutoModel.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "#         # Set up token classification head\n",
    "#         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             self.bert.pooler,\n",
    "#             nn.Linear(config.hidden_size, 1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "#         # Load and initialize weights\n",
    "#         self.init_weights()\n",
    "        \n",
    "#     def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,labels=None, **kwargs):\n",
    "#         # Use model body to get encoder representations\n",
    "#         outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, **kwargs)\n",
    "#         # Apply classifier to encoder representation\n",
    "#         sequence_output = self.dropout (outputs[1])\n",
    "#         logits = self.classifier(sequence_output)\n",
    "#         # Calculate losses\n",
    "#         loss = None\n",
    "#         if labels is not None:\n",
    "#             loss_fct = nn.CrossEntropyLoss()\n",
    "#             loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "#         # Return model output object\n",
    "#         return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertModel, BertForSequenceClassification\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class BertForSigmoidSequenceClassification(BertForSequenceClassification):\n",
    "#     def init(self, config):\n",
    "#         super(BertForSigmoidSequenceClassification, self).init(config)\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             self.bert.pooler,\n",
    "#             nn.Linear(config.hidden_size, config.num_labels),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
    "#         outputs = self.bert(\n",
    "#             input_ids=input_ids,\n",
    "#             attention_mask=attention_mask,\n",
    "#             token_type_ids=token_type_ids\n",
    "#         )\n",
    "#         pooled_output = outputs[1]\n",
    "#         logits = self.classifier(pooled_output)\n",
    "\n",
    "#         loss = None\n",
    "#         if labels is not None:\n",
    "#             loss_fct = nn.BCELoss()\n",
    "#             labels = labels.float()  # Convert labels to float for BCE loss\n",
    "#             loss = loss_fct(logits.view(-1, 1), labels.view(-1, 1))\n",
    "\n",
    "#         return logits, loss\n",
    "\n",
    "# # Example usage\n",
    "# model_name = \"bert-base-uncased\"  # Replace with your desired BERT model\n",
    "# config = BertForSequenceClassification.from_pretrained(model_name).config\n",
    "# model = BertForSigmoidSequenceClassification(config)\n",
    "\n",
    "# # Pass your input data through the model\n",
    "# input_ids = torch.tensor([ ... ], dtype=torch.long)  # Your input token IDs\n",
    "# attention_mask = torch.tensor([ ... ], dtype=torch.long)  # Your attention mask\n",
    "# logits, loss = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Use logits for prediction or loss for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "# custom trainer method to implement our loss function\n",
    "class CustomTrainer(Trainer):\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Extract all necessary inputs\n",
    "        labels = inputs.pop(\"reward\")\n",
    "        policy_loss = inputs.pop(\"policy_loss\")\n",
    "        value_loss = inputs.pop(\"value_loss\")\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        loss = bert_loss(logits, labels, policy_loss, value_loss)\n",
    "        loss = torch.mean(loss)\n",
    "        if return_outputs:\n",
    "            return (loss, outputs)\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "# SON_Config = AutoConfig.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "# def model_init():\n",
    "#     return SON_BERT.from_pretrained(\"prajjwal1/bert-tiny\", config=SON_Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`prediction` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:731\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 731\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Assuming you have a formatted_dataset which is a Dataset object\u001b[39;00m\n\u001b[1;32m      6\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(formatted_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, collate_fn\u001b[38;5;241m=\u001b[39mdata_collator)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Check the first batch only\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/data/data_collator.py:249\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 249\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m    257\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3074\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3071\u001b[0m             batch_outputs[key] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3072\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[0;32m-> 3074\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:211\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    207\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:747\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    743\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    744\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    745\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    746\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 747\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    748\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    749\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    750\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    751\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    752\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`prediction` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "\n",
    "# Assuming you have a formatted_dataset which is a Dataset object\n",
    "loader = DataLoader(formatted_dataset, batch_size=1, collate_fn=data_collator)\n",
    "for batch in loader:\n",
    "    print(batch['input_ids'].shape, batch['attention_mask'].shape)  # Check the first batch only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`prediction` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:731\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 731\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 45\u001b[0m\n\u001b[1;32m     17\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CustomTrainer(\n\u001b[1;32m     18\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     19\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     20\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer_1,\n\u001b[1;32m     21\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_test_split[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# training_args = TrainingArguments(\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#     output_dir=\"./models/bert_reward_model\",\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#     per_device_train_batch_size=4,  # Smaller batch size\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#     # compute_metrics=lambda eval_pred: {\"loss\": eval_pred.loss}  # Custom metrics function if needed\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1536\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1537\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1538\u001b[0m )\n\u001b[0;32m-> 1539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1787\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1784\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1786\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1787\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1788\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1789\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/data_loader.py:384\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 384\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/data/data_collator.py:249\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 249\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m    257\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3074\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3071\u001b[0m             batch_outputs[key] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3072\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[0;32m-> 3074\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:211\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    207\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:747\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    743\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    744\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    745\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    746\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 747\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    748\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    749\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    750\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    751\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    752\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`prediction` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Configuring the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/bert_reward_model_v2\",\n",
    "    per_device_train_batch_size=1,\n",
    "    evaluation_strategy=\"no\",\n",
    "    logging_steps=1,\n",
    "    num_train_epochs = 10,\n",
    "    report_to=None,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "# use new trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer_1,\n",
    "    train_dataset=train_test_split['train']\n",
    ")\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./models/bert_reward_model\",\n",
    "#     per_device_train_batch_size=4,  # Smaller batch size\n",
    "#     per_device_eval_batch_size=4,   # Smaller batch size for evaluation\n",
    "#     evaluation_strategy=\"epoch\",    # Change to 'epoch' to evaluate after each epoch\n",
    "#     logging_steps=10,               # Adjust based on the number of steps per epoch\n",
    "#     num_train_epochs=20,            # You might need more epochs due to fewer data per epoch\n",
    "#     save_strategy=\"epoch\",          # Save the model at the end of each epoch\n",
    "#     load_best_model_at_end=True,    # Load the best model based on validation loss at the end\n",
    "#     metric_for_best_model=\"loss\",   # Use loss to determine the best model\n",
    "#     report_to=None                  # Avoid logging to any online service\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_test_split['train'],\n",
    "#     eval_dataset=train_test_split['test'],\n",
    "#     # compute_metrics=lambda eval_pred: {\"loss\": eval_pred.loss}  # Custom metrics function if needed\n",
    "# )\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# load BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Specify the model directory or name\n",
    "model_directory = \"./models/bert_reward_model/checkpoint-1000\"  # Adjust path as necessary\n",
    "\n",
    "# Load trained model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_directory)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "\n",
    "def prepare_input(query, response):\n",
    "    # Tokenize the query and response\n",
    "    inputs = tokenizer(query, response, \n",
    "                       padding=\"max_length\", \n",
    "                       truncation=True, \n",
    "                       max_length=128, \n",
    "                       return_tensors=\"pt\")  # \"pt\" for PyTorch tensors\n",
    "    return inputs\n",
    "\n",
    "def make_prediction(query, response):\n",
    "    # Prepare the input data\n",
    "    inputs = prepare_input(query, response)\n",
    "\n",
    "    # Move the tensors to the same device as the model\n",
    "    inputs = {key: value.to(model.device) for key, value in inputs.items()}\n",
    "\n",
    "    # Get the model's prediction\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract logits\n",
    "    logits = outputs.logits\n",
    "\n",
    "    if model.config.num_labels == 1:\n",
    "        prediction = logits.squeeze().item()  # Get the single value from the tensor\n",
    "        \n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize storage for dataset\n",
    "training_data = {\n",
    "    \"prediction\": [],\n",
    "    \"groundtruth\": [],\n",
    "    \"reward\": [],\n",
    "    \"policy_loss\":[],\n",
    "    \"value_loss\":[]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 43221,
     "status": "ok",
     "timestamp": 1697492013409,
     "user": {
      "displayName": "Luca Podo",
      "userId": "02050102078660879808"
     },
     "user_tz": -120
    },
    "id": "MG3D2YEhtlYn",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "65287784-530a-4059-94bb-3d4528a2187f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "save_freq = 100\n",
    "empty = 0\n",
    "\n",
    "current_stats = {}\n",
    "prev_stats = {}\n",
    "for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader, start=0)):\n",
    "    \n",
    "    \n",
    "    index = df[df['query'] == batch['query'][0]].index[0]\n",
    "    groundtruth = df.loc[[index]]['response'].values[0]\n",
    "    \n",
    "    #tmp_df = getDataframe(batch['query'][0])\n",
    "\n",
    "    if epoch >= 800:\n",
    "        break\n",
    "\n",
    "    question_tensors = batch[\"input_ids\"]\n",
    "    response_tensors = ppo_trainer.generate(\n",
    "        question_tensors,\n",
    "        return_prompt=False,\n",
    "        length_sampler=output_length_sampler,\n",
    "        **generation_kwargs,\n",
    "    )\n",
    "    \n",
    "    batch[\"response\"] = tokenizer_.batch_decode(response_tensors, skip_special_tokens=True)\n",
    "    #reward_number, pred = get_newton_score(batch[\"response\"][0],groundtruth)\n",
    "    prediction = batch[\"response\"][0]\n",
    "    \n",
    "    prediction = prediction.strip()\n",
    "    prediction = \"mark \" + prediction\n",
    "    groundtruth = groundtruth.strip()\n",
    "    \n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    print(f\"Groundtruth: {groundtruth}\")\n",
    "    \n",
    "    ######\n",
    "    # observer model trained from scratch\n",
    "    #####\n",
    "    # prediction_ids = tokenizer_(prediction, padding=\"max_length\", max_length=50, truncation=True)['input_ids']\n",
    "    # groundtruth_ids = tokenizer_(groundtruth, padding=\"max_length\", max_length=50, truncation=True)['input_ids']\n",
    "    # feature_a = torch.tensor([prediction_ids], dtype=torch.long)\n",
    "    # feature_b = torch.tensor([groundtruth_ids], dtype=torch.long)\n",
    "    # concatenated_input_ids = prediction_ids + groundtruth_ids\n",
    "    # features = torch.tensor([concatenated_input_ids], dtype=torch.long)\n",
    "    # observer_output = observer_self_attn(feature_a, feature_b)\n",
    "    \n",
    "    #####\n",
    "    #TinyBert fine-tuning\n",
    "    #####\n",
    "    # inputs = tokenizer(prediction, groundtruth, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
    "    # inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    # # Forward pass: get the logits from TinyBERT\n",
    "    # with torch.no_grad():  # Assuming you're not updating TinyBERT based on PPO model's predictions directly\n",
    "    #     outputs = model(**inputs)\n",
    "    # # Calculate the predicted reward: assuming the output is a regression value\n",
    "    # bert_output = outputs.logits.squeeze().item()\n",
    "    \n",
    "    # bert_output = make_prediction(prediction, groundtruth)\n",
    "    \n",
    "    score= 0\n",
    "    score = get_newton_score(prediction, groundtruth)\n",
    "    # print(f\"predicted reward: {observer_output}, discrete reward: {score}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # if epoch <= 50:\n",
    "    #     rewards = [torch.tensor(score, dtype=torch.float)]\n",
    "    # else:\n",
    "    #     rewards = [torch.tensor(observer_output, dtype=torch.float)]\n",
    "    rewards = [torch.tensor(score, dtype=torch.float)]\n",
    "    # print(rewards)\n",
    "    prev_stats = current_stats\n",
    "    \n",
    "    stats = ppo_trainer.step(question_tensors, response_tensors, rewards)\n",
    "    \n",
    "#     if epoch > 0 and epoch <= 201:\n",
    "#         training_data['prediction'].append(prediction)  # Storing the original prompt\n",
    "#         training_data['groundtruth'].append(groundtruth)         # Storing the generated response\n",
    "#         policy_loss_diff = stats.get('ppo/loss/policy')[0] - prev_stats.get('ppo/loss/policy')[0]\n",
    "#         value_loss_diff = stats.get('ppo/loss/value')[0] - prev_stats.get('ppo/loss/value')[0]\n",
    "#         training_data['policy_loss'].append(policy_loss_diff)\n",
    "#         training_data['value_loss'].append(value_loss_diff)\n",
    "#         training_data['reward'].append(score)  \n",
    "#     if epoch == 202:\n",
    "#         # Convert the dictionary to DataFrame\n",
    "#         df_train = pd.DataFrame(training_data)\n",
    "\n",
    "#         # Save to a CSV file for later use\n",
    "#         df_train.to_csv(\"./training_data_easy.csv\", index=False)\n",
    "    \n",
    "    # current_stats = stats\n",
    "    # if epoch % 1 == 0 and epoch > 0:\n",
    "    #     ##\n",
    "    #     # model trained from scratch\n",
    "    #     ##\n",
    "    #     observer_loss = calculate_observer_loss_with_differences(observer_output, score, current_stats, prev_stats, weights)\n",
    "    #     # Backpropagate loss\n",
    "    #     optimizer_self_attn.zero_grad()\n",
    "    #     observer_loss.backward()\n",
    "    #     optimizer_self_attn.step()\n",
    "        # lr_scheduler.step(epoch)\n",
    "        ####\n",
    "        #tiny bert\n",
    "        ####\n",
    "        # Calculate loss\n",
    "        # bert_tensor = torch.tensor(bert_output, dtype=torch.float, requires_grad=True)\n",
    "        # loss = calculate_observer_loss_with_differences(bert_tensor, score, current_stats, prev_stats, weights)\n",
    "        # optimizer_bert.zero_grad()\n",
    "        # loss.backward()\n",
    "        # optimizer_bert.step()\n",
    "\n",
    "\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)\n",
    "   # else:\n",
    "   #     print('errore response vuota')\n",
    "   #     empty +=1\n",
    "    \n",
    "    #if save_freq and epoch and epoch % save_freq == 0:\n",
    "    #    ppo_trainer.save_pretrained(\"/content/drive/MyDrive/DeepvizLab/NewtonLLM/checkpoints/\" + f\"step_{epoch}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_trainer.save_pretrained('./models/200-discrete')\n",
    "tokenizer_.save_pretrained('./models/200-discrete-tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"DeepvizLab/newton-7b\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, \"./models/200-easy-baseline\")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained('./models/200-easy-tokenizer-baseline', trust_remote_code=True)\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "#tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = load_dataset(\"LucaPodo/newton-dataset-v1\",split=\"test\", revision=\"0.0.2\",download_mode=\"force_redownload\", cache_dir=\"./tmp\" )\n",
    "def remove_reponse(sample):\n",
    "    sample['hardness'] = sample['text'].split(\"@\")[0].strip()\n",
    "    text = sample['text'].split('@')[1]\n",
    "\n",
    "    sample['response'] = text.split(\"### Response:\")[1].strip()\n",
    "    sample['text'] = (text.split(\"### Response:\")[0] + \"### Response: mark\\n\").strip()\n",
    "    return sample\n",
    "\n",
    "ds = ds.map(remove_reponse, batched=False)\n",
    "\n",
    "print(ds)\n",
    "\n",
    "test = ds.filter(lambda ds: ds['hardness'] == \"Extra Hard\" or ds['hardness'] == \"Extra Hard\")\n",
    "\n",
    "print(test[1])\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=400, eos_token_id=tokenizer.eos_token_id)\n",
    "# test = test.select(range(200, len(ds)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "good = 0\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range(len(test))):\n",
    "\n",
    "    prompt = test[i]['text']\n",
    "    groundtruth = test[i]['response']\n",
    "    # print(groundtruth + '\\n')\n",
    "    result = pipe(prompt)\n",
    "    prediction = result[0]['generated_text'].split(\"### Response:\")[1].strip()\n",
    "    # print(prediction)\n",
    "    cc = can_compile(prediction, groundtruth)\n",
    "    # print(cc)\n",
    "    # print('///////////////////////')\n",
    "    if cc:\n",
    "        good += 1\n",
    "    \n",
    "print(f\"{good} / {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"DeepvizLab/newton-7b\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, \"./models/newton-7b-100\")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained('./models/newton-1k-tokenizer-raff', trust_remote_code=True)\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "#tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_valid['text'][0])\n",
    "a = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quando si modifica il modello si deve successivamente rifare il resize\n",
    "tokenizer.padding_side = \"left\"\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "login(token=\"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\")\n",
    "\n",
    "model.push_to_hub(\"DeepvizLab/newton-7b-rl-raff\")\n",
    "tokenizer.push_to_hub(\"DeepvizLab/newton-7b-token-raff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-north-1:243637512696:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "lcc_arn": "arn:aws:sagemaker:eu-north-1:490853539869:studio-lifecycle-config/my-studio-lcc"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
